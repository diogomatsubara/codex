===============
OpenStack Guide
===============

Overview
--------

Welcome to the Stark & Wayne guide to deploying Cloud Foundry on
OpenStack. This guide provides the steps to create authentication
credentials, generate the underlying cloud infrastructure, then use
Terraform to prepare a bastion host.

From this bastion, we setup a special BOSH Director we call the
**proto-BOSH** server where software like Vault, Concourse, Bolo and
SHIELD are setup in order to give each of the environments created after
the **proto-BOSH** key benefits of:

-  Secure Credential Storage
-  Pipeline Management
-  Monitoring Framework
-  Backup and Restore Datastores

Once the **proto-BOSH** environment is setup, the child environments
will have the added benefit of being able to update their BOSH software
as a release, rather than having to re-initialize with ``bosh-init``.

This also increases the resiliency of all BOSH Directors through
monitoring and backups with software created by Stark & Wayne's
engineers.

And visibility into the progress and health of each application,
release, or package is available through the power of Concourse
pipelines.

.. image:: /images/levels_of_bosh.png
   :alt: Levels of Bosh

BOSH (3) are the per-site BOSH Directors. Note that it is the proto-BOSH

well as the other BOSH directors.

Now it's time to setup the credentials.

Setup Credentials
-----------------

To start deploying the infrastructure, the first thing you need to do is
create an OpenStack user and give it admin access to a new tenant.

1. Log into Horizon as the **admin** user.
2. Under **Identity --> Projects**, create a new project and set the
   quotas to accommodate the environment(s) that will be deployed to
   this project.

Keep in mind: If setting specific quotas, you will need to ask for more
than the total resources that you are going to end up with. This is to
account for transient instances, such as BOSH errands and Compilation
instances.

For a typical development environment: \* **VCPUs** - 200 \*
**Instances** - 100 \* **Volumes** - 25 \* **RAM (MB)** - 512000

For a typical staging / production environment: \* **VCPUs** - 200 \*
**Instances** - 100 \* **Volumes** - 25 \* **RAM (MB)** - 512000

3. Under **Identity --> Users**, create a new user. Give the user
   **admin** access to the new project.

Generate OpenStack Key Pair
~~~~~~~~~~~~~~~~~~~~~~~~~~~

The **User Name**, **Password**, and **Tenant Name** (same as **Project
Name**) are used to get access to the OpenStack Services by BOSH. Next,
we'll need to create a **Key Pair**. This will be used as we bring up
the initial bastion host instances, and is the SSH key you'll use to
connect from your local machine to the bastion.

**NOTE:** Due to a bug in Liberty and Mitaka you cannot use keys
generated by Nova with bosh-init: you must generate the key pair
separately with ``ssh-keygen``. Releases of OpenStack before Liberty and
after Mitaka are not impacted by this issue, so keys can either be
generated in OpenStack or with ``ssh-keygen``. A short description of
this error is available on the `bosh.io openstack-cpi
doc <https://bosh.io/docs/openstack-cpi.html>`__ with a link to the bug
report.

1. Log into Horizon as the user that has admin access to the project in
   question.

-  Make sure you are in the correct project (top-left corner of the
   Horizon UI) or the key pair you generate will not work.

2. Under **Project --> Compute --> Access & Security**, head to the
   **Key Pairs** tab.

3. **To create a key in OpenStack:** Click **Create Key Pair** and give
   the key a suitable name, e.g. ``bosh``. When the key pair is
   generated it will be immediately available for use in OpenStack and
   its ``*.pem`` file will download. **To use a key pair created with
   ``ssh-keygen``:** Click **Import Key Pair**, give the key a suitable
   name (e.g. ``bosh``), and paste in the public key.

4. ``*.pem`` / key files should have their permissions set to ``0600``.

5. Decide where you want this file to be. All ``*.pem`` files are
   ignored in the codex repository. So you can either move this file to
   the same folder as ``CODEX_ROOT/terraform/openstack`` or move it to a
   place you normally keep SSH keys

Stand Up A Public Network
-------------------------

Though Terraform is used to stand up most of the networking and initial
infrastructure, it does not set up the publicly facing network that is
used to provide external access to instances and floating IP addresses.
To do this, use ``neutron net-create``, ``neutron subnet-create``, to
create the public network and allocate the pool of publicly facing IP
addresses, some of which will be used as floating IP's.

Some sample commands are as follows (these will vary depending on the
physical structure of your network, the configuration of neutron, and
whether or not you are using a 3rd party SDN plugin):

::

    neutron net-create public --provider:network_type vlan --provider:physical_network physnet1 --provider:segmentation_id 100 --tenant-id=3c155e766d1d44718cd35765c709fae1 --shared --router:external=True
    neutron subnet-create public    192.168.10.0/24 --disable-dhcp --allocation-pool start=192.168.10.110,end=192.168.10.250 --gateway 192.168.10.1

Make note of the public network's UUID. It will be needed in the next
step.

Use Terraform
-------------

Once the requirements for OpenStack are met, we can put it all together
and build out your shiny new networks, routers, security groups and
bastion host. Before we begin, copy the contents of
``CODEX_ROOT/terraform/openstack`` to your own Github repository and
then change to that directory.

The configuration directly matches the `Network
Plan <https://github.com/starkandwayne/codex/blob/master/network.md>`__
for the demo environment. When deploying in other environments like
production, some tweaks or rewrites may need to be made.

Variable File
~~~~~~~~~~~~~

Create an ``openstack.tfvars`` file with the following configurations
(substituting your actual values) all the other configurations have
default setting in the ``openstack.tf`` file.

::

    tenant_name = "cf"
    user_name = "cfadmin"
    password = "putyourpasswordhere"
    auth_url = "http://identity.mydatacenter.io:5000/v2.0"
    key_pair = "bosh"
    region = "os-dc1"
    pub_net_uuid = "09b03d93-45f8-4bea-b3b8-7ad9169f23d5"

If you need to change the region or subnet, you can override the
defaults by adding:

::

    region     = "RegionTwo"
    network    = "10.42"

To see what variables can be overridden in ``openstack.tfvars``, and
their default values, please look at the top of ``openstack.tf``.

You may change some default settings according to the real cases you are
working on. For example, you can change ``flavor_id`` (default is ``3``,
which is m1.medium) in ``openstack.tf`` to something larger if the
bastion would require a high workload.

Build Resources
~~~~~~~~~~~~~~~

As a quick pre-flight check, run ``make manifest`` to compile your
Terraform plan and suss out any issues with naming, missing variables,
configuration, etc.:

::

    $ make manifest
    terraform get -update
    terraform plan -var-file openstack.tfvars -out openstack.tfplan
    Refreshing Terraform state prior to plan...

    <snip>

    Plan: 129 to add, 0 to change, 0 to destroy.

If everything worked out you should see a summary of the plan. If this
is the first time you've done this, all of your changes should be
additions. The numbers may differ from the above output, and that's
okay.

Now, to pull the trigger, run ``make deploy``:

::

    $ make deploy

Terraform will connect to OpenStack using the credentials you provided
in the \*.tfvars file and spin up all the things it needs. When it
finishes, you should be left with a bunch of subnets, security groups,
and a bastion host.

**NOTE:** Circling back around, once you have deployed your Terraform
configuration you will need to push it to your Github repository. When
you ran ``make manifest`` Terraform created a \*.tfplan file which is
based on not only the current \*.tf and \*.tfvars files but also on the
\*.tfstate file from the last run. If there is no \*.tfstate file then
Terraform will assume you are starting a new configuration. When making
changes in the future, you'll want to make sure to push them after the
``make deploy`` to ensure that the \*.tfstate file you have in Github is
current and correct.

Connect to Bastion
~~~~~~~~~~~~~~~~~~

You'll use the **Key Pair** ``*.pem`` or ``ssh-keygen`` generated file
that was stored from the `Generate OpenStack Key
Pair <openstack.md#generate-openstack-key-pair>`__ step before as your
credential to connect.

In forming the SSH connection command, use the ``-i`` flag to give SSH
the path to the ``IdentityFile``. The default user on the bastion server
is ``ubuntu``. This will change in a little bit though when we create a
new user, so don't get too comfy.

::

    $ ssh -i ~/.ssh/bosh ubuntu@192.168.10.117

Add User
~~~~~~~~

Once on the bastion host, you'll need to install the ``jumpbox`` script:

::

    sudo curl -o /usr/local/bin/jumpbox \
      https://raw.githubusercontent.com/starkandwayne/jumpbox/master/bin/jumpbox
    sudo chmod 0755 /usr/local/bin/jumpbox

To check if ``jumpbox`` was installed correctly, try checking the
version:

::

    $ jumpbox -v
    jumpbox v50

`This script installs <https://github.com/starkandwayne/jumpbox>`__ some
useful utilities such as ``jq``, ``spruce``, ``safe``, and ``genesis`` -
all of which will be important when we start using the bastion host to
do deployments.

**NOTE**: Try not to confuse the ``jumpbox`` script with the jumpbox
*BOSH release*. The *BOSH release* can be used as part of a deployment
whereas the script is run directly on the bastion host.

In order to have the dependencies for the ``bosh_cli`` we need to create
a user. As part of creating a user you will be prompted for their git
configuration, which will be useful when we use Genesis templates to
create deployments later.

Using named accounts will also provide auditing (via the ``sudo`` logs)
as well as isolation (people won't step on each others toes on the
filesystem) and customization (everyone gets to set their own prompt /
shell / ``$EDITOR``).

Let's add a user with ``jumpbox useradd``:

::

    $ jumpbox useradd
    Full name: J User
    Username:  juser
    Enter the public key for this user's .ssh/authorized_keys file:
    You should run `jumpbox user` now, as juser:
      su - juser
      jumpbox user

Setup User
~~~~~~~~~~

After you've added the user, **be sure you follow up and setup the
user** before going any further.

Use the ``su - juser`` command to switch to the user. And run
``jumpbox user`` to install all dependent packages.

::

    $ su - juser
    $ jumpbox user

The following warning may show up when you run ``jumpbox user``:

::

     * WARNING: You have '~/.profile' file, you might want to load it,
        to do that add the following line to '/home/juser/.bash_profile':

          source ~/.profile

In this case, please follow the ``WARNING`` message, otherwise you may
see the following message when you run ``jumpbox`` command even if you
already installed everything when you run ``jumpbox user``.

::

    ruby not installed
    rvm not installed
    bosh not installed

SSH Config
~~~~~~~~~~

On your local computer, setup an entry in the ``~/.ssh/config`` file for
your bastion host - substituting the correct IP and SSH key.

::

    Host bastion
      Hostname 52.43.51.197
      IdentityFile ~/.ssh/id_rsa
      User juser

Test Login
~~~~~~~~~~

After you've logged in as ``ubuntu`` once, created your user, logged
out, and configured your SSH config, you'll be ready to try to connect
via the ``Host`` alias.

::

    $ ssh bastion

If you can login and run ``jumpbox`` and everything returns green,
everything's ready to continue.

::

    $ jumpbox

    <snip>

    >> Checking jumpbox installation
    jumpbox installed - jumpbox v49
    ruby installed - ruby 2.2.4p230 (2015-12-16 revision 53155) [x86_64-linux]
    rvm installed - rvm 1.27.0 (latest) by Wayne E. Seguin <wayneeseguin@gmail.com>, Michal Papis <mpapis@gmail.com> [https://rvm.io/]
    bosh installed - BOSH 1.3184.1.0
    bosh-init installed - version 0.0.81-775439c-2015-12-09T00:36:03Z
    jq installed - jq-1.5
    spruce installed - spruce - Version 1.7.0
    safe installed - safe v0.0.23
    vault installed - Vault v0.6.0
    genesis installed - genesis 1.5.2 (61864a21370c)

    git user.name  is 'J User'
    git user.email is 'juser@starkandwayne.com'

Proto Environment
-----------------

.. image:: /images/global_network_diagram.png
   :alt: Global Network Diagram

-  Global
-  Site
-  Environment

Site Name
~~~~~~~~~

Sometimes the site level name can be a bit tricky because each IaaS
divides things differently. With OpenStack we suggest a default of the
OpenStack Datacenter you're using, for example: ``dc01``.

Environment Name
~~~~~~~~~~~~~~~~

All of the software the **proto-BOSH** will deploy will be in the
``proto`` environment. And by this point, you've setup your credentials
and used Terraform to construct the IaaS components and configure your
bastion host. We're ready now to setup a BOSH Director on the bastion.

The first step is to create a **vault-init** process.

vault-init
~~~~~~~~~~

.. image:: /images/bastion_step_1.png
   :alt: vault-init

BOSH has secrets. Lots of them. Components like NATS and the database
rely on secure passwords for inter-component interaction. Ideally, we'd
have a spinning Vault for storing our credentials, so that we don't have
them on-disk or in a git repository somewhere.

However, we are starting from almost nothing, so we don't have the
luxury of using a BOSH-deployed Vault. What we can do, however, is spin
a single-threaded Vault server instance **on the bastion host**, and
then migrate the credentials to the real Vault later.

This we call a **vault-init**. Because it precedes the **proto-BOSH**
and Vault deploy we'll be setting up later.

The ``jumpbox`` script that we ran as part of setting up the bastion
host installs the ``vault`` command-line utility, which includes not
only the client for interacting with Vault (``safe``), but also the
Vault server daemon itself.

Start Server
^^^^^^^^^^^^

Were going to start the server and do an overview of what the output
means. To start the **vault-init**, run the ``vault server`` with the
``-dev`` flag.

**NOTE**: When you run the ``vault server -dev`` command, we recommend
running it in the foreground using either a ``tmux`` session or a
separate ssh tab. Also, we do need to capture the output of the
``Root Token``.

::

    $ vault server -dev
    ==> WARNING: Dev mode is enabled!

    In this mode, Vault is completely in-memory and unsealed.
    Vault is configured to only have a single unseal key. The root
    token has already been authenticated with the CLI, so you can
    immediately begin using the Vault CLI.

A vault being unsealed sounds like a bad thing right? But if you think
about it like at a bank, you can't get to what's in a vault unless it's
unsealed.

And in dev mode, ``vault server`` gives the user the tools needed to
authenticate. We'll be using these soon when we log in.

::

    The unseal key and root token are reproduced below in case you
    want to seal/unseal the Vault or play with authentication.

    Unseal Key:
    781d77046dcbcf77d1423623550d28f152d9b419e09df0c66b553e1239843d89
    Root Token: c888c5cd-bedd-d0e6-ae68-5bd2debee3b7

Setup vault-init
^^^^^^^^^^^^^^^^

In order to setup the **vault-init** we need to target the server and
authenticate. We use ``safe`` as our CLI to do both commands.

The local ``vault server`` runs on ``127.0.0.1`` and on port ``8200``.

::

    $ safe target init http://127.0.0.1:8200
    Now targeting init at http://127.0.0.1:8200

    $ safe targets

      init  http://127.0.0.1:8200

Authenticate with the ``Root Token`` from the ``vault server`` output.

::

    $ safe auth token
    Authenticating against init at http://127.0.0.1:8200
    Token: <paste your Root Token here>

Test vault-init
^^^^^^^^^^^^^^^

Here's a smoke test to see if you've setup the **vault-init** correctly.

::

    $ safe set secret/handshake knock=knock
    knock: knock

    $ safe get secret/handshake
    --- # secret/handshake
    knock: knock

**NOTE**: If you receive ``API 400 Bad Request`` when attempting
``safe set``, you may have incorrectly copied and entered your Root Key.
Try ``safe auth token`` again.

All set! Now we can now build our deploy for the **proto-BOSH**.

proto-BOSH
~~~~~~~~~~

.. image:: /images/bastion_step_2.png
   :alt: proto-BOSH

Generate BOSH Deploy
^^^^^^^^^^^^^^^^^^^^

When using `the Genesis
framework <https://github.com/starkandwayne/genesis>`__ to manage our
deploys across environments, each deployment will need its own
repository. For each new deployment, you'll need to change the remote
URL in the ``.git/config`` file.

First setup a ``ops`` folder in your user's home directory.

::

    $ mkdir -p ~/ops
    $ cd ~/ops

Genesis has a template for BOSH deployments (including support for the
**proto-BOSH**), so let's use that by passing ``bosh`` into the
``--template`` flag.

::

    $ genesis new deployment --template bosh
    $ cd ~/ops/bosh-deployments

Next, we'll create a site and an environment from which to deploy our
**proto-BOSH**. The BOSH template comes with some site templates to help
you get started quickly, including:

-  ``aws`` for Amazon Web Services VPC deployments
-  ``vsphere`` for VMWare ESXi virtualization clusters
-  ``openstack`` for OpenStack tenant deployments

When generating a new site we'll use this command format:

::

    genesis new site --template <name> <site_name>

The template ``<name>`` will be ``openstack`` because that's our IaaS
we're working with and we recommend the ``<site_name>`` default to the
OpenStack Datacenter, ex. ``dc01``.

::

    $ genesis new site --template openstack dc01
    Created site dc01 (from template openstack):
    ~/ops/bosh-deployments/openstack
    ├── README
    └── site
        ├── README
        ├── disk-pools.yml
        ├── jobs.yml
        ├── networks.yml
        ├── properties.yml
        ├── releases
        ├── resource-pools.yml
        ├── stemcell
        │   ├── name
        │   ├── sha1
        │   ├── url
        │   └── version
        └── update.yml

    2 directories, 13 files

Finally, let's create our new environment, and name it ``proto`` (that's
``dc01/proto``, formally speaking).

::

    $ genesis new env --type bosh-init dc01 proto
    Running env setup hook: ~/ops/bosh-deployments/.env_hooks/setup

     init  http://127.0.0.1:8200

    Use this Vault for storing deployment credentials?  [yes or no]
    yes
    Setting up credentials in vault, under secret/dc01/proto/bosh
    .
    └── secret/dc01/proto/bosh
        ├── blobstore/
        │   ├── agent
        │   └── director
        ├── db
        ├── nats
        ├── users/
        │   ├── admin
        │   └── hm
        └── vcap


    Created environment dc01/:
    ~/ops/bosh-deployments/dc01/proto
    ├── credentials.yml
    ├── Makefile
    ├── name.yml
    ├── networking.yml
    ├── properties.yml
    └── README

    0 directories, 6 files

**NOTE** Don't forget that ``--type bosh-init`` flag is very important.
Otherwise, you'll run into problems with your deployment.

The template helpfully generated all new credentials for us and stored
them in our **vault-init**, under the ``secret/dc01/proto/bosh``
subtree. Later, we'll migrate this subtree over to our real Vault, once
it is up and spinning.

Make Manifest
^^^^^^^^^^^^^

Let's head into the ``proto/`` environment directory and see if we can
create a manifest or (a more likely case) we still have to provide some
critical information:

::

    $ cd ~/ops/bosh-deployments/dc01/proto
    $ make manifest
    9 error(s) detected:
     - $.cloud_provider.properties.openstack.default_key_name: What is your full key name?
     - $.cloud_provider.properties.openstack.default_security_groups: What Security Groups?
     - $.cloud_provider.ssh_tunnel.private_key: What is the local path to the Private Key for this deployment?  Due to a bug in Openstack Liberty and Mitaka, you need to use an SSH key generated by ssh-keygen, not one generated by Nova.
     - $.meta.openstack.api_key: Please supply an Openstack password
     - $.meta.openstack.auth_url: Please supply the authentication URL for the Openstack Identity Service
     - $.meta.openstack.tenant: Please supply an Openstack tenant name
     - $.meta.openstack.username: Please supply an Openstack user name
     - $.meta.shield_public_key: Specify the SSH public key from this environment's SHIELD daemon
     - $.networks.default.subnets: Specify subnets for your BOSH vm's network
    Availability Zone will BOSH be in?


    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

Let's focus on the ``$.meta`` subtree, since that's where most
parameters are defined in Genesis templates:

::

    - $.meta.openstack.api_key: Please supply an Openstack password
    - $.meta.openstack.auth_url: Please supply the authentication URL for the Openstack Identity Service
    - $.meta.openstack.tenant: Please supply an Openstack tenant name
    - $.meta.openstack.username: Please supply an Openstack user name

This is easy enough to supply. We'll put these properties in
``properties.yml``:

::

    ---
    meta:
      openstack:
        api_key:  (( vault meta.vault_prefix "/openstack:api_key" ))
        tenant:   (( vault meta.vault_prefix "/openstack:tenant" ))
        username: (( vault meta.vault_prefix "/openstack:username" ))
        auth_url: http://identity.mydatacenter.io:5000/v2.0
        region: os-dc1

Configure the OpenStack credentials by pointing Genesis to the Vault.
Let's go put those credentials in the Vault:

::

    $ export VAULT_PREFIX=secret/dc01/proto/os-dc1
    $ safe set ${VAULT_PREFIX}/openstack tenant=cf username=cfadmin api_key=putyourpasswordhere

Let's try that ``make manifest`` again.

::

    $ make manifest
    5 error(s) detected:
    - $.cloud_provider.properties.openstack.default_key_name: What is your full key name?
    - $.cloud_provider.properties.openstack.default_security_groups: What Security Groups?
    - $.cloud_provider.ssh_tunnel.private_key: What is the local path to the Private Key for this deployment?  Due to a bug in Openstack Liberty and Mitaka, you need to use an SSH key generated by ssh-keygen, not one generated by Nova.
    - $.meta.shield_public_key: Specify the SSH public key from this environment's SHIELD daemon
    - $.networks.default.subnets: Specify subnets for your BOSH vm's network


    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

Better. Let's configure our ``cloud_provider`` for OpenStack, using our
OpenStack key pair. We need copy our private key to the bastion host and
path to the key for the ``private_key`` entry in the following
``properties.yml``.

On your local computer, you can copy to the clipboard with the
``pbcopy`` command on a macOS machine:

::

    cat ~/.ssh/bosh.pem | pbcopy
    <paste values to /path/to/the/openstack/key.pem>

Then add the following to the ``properties.yml`` file.

::

    ---
    meta:
    ...
    cloud_provider:
      properties:
        openstack:
          default_key_name: bosh
          connection_options:
            connect_timeout: 600
          ignore_server_availability_zone: true
      ssh_tunnel:
        host: (( grab jobs.bosh.networks.default.static_ips.0 ))
        private_key: ~/.ssh/bosh

Note here the ``ignore_server_availability_zone``. This setting needs to
be set to ``true`` if the AZ for Cinder is not the same as the one for
Nova. Otherwise, BOSH will have difficulty creating block storage
volumes.

Once more, with feeling:

::

    $ make manifest
    3 error(s) detected:
     - $.cloud_provider.properties.openstack.default_security_groups: What Security Groups?
     - $.meta.shield_public_key: Specify the SSH public key from this environment's SHIELD daemon
     - $.networks.default.subnets: Specify subnets for your BOSH vm's network


    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

Excellent. We're down to three issues.

We haven't deployed a SHIELD yet, so it may seem a bit odd that we're
being asked for an SSH public key. When we deploy our **proto-BOSH** via
``bosh-init``, we're going to spend a fair chunk of time compiling
packages on the bastion host before we can actually create and update
the director VM. ``bosh-init`` will delete the director VM before it
starts this compilation phase, so we will be unable to do *anything*
while ``bosh-init`` is hard at work. The whole process takes about 30
minutes, so we want to minimize the number of times we have to re-deploy
**proto-BOSH**. By specifying the SHIELD agent configuration up-front,
we skip a re-deploy after SHIELD itself is up.

Let's leverage our Vault to create the SSH key pair for BOSH. ``safe``
has a handy builtin for doing this:

::

    $ safe ssh secret/dc01/proto/shield/keys/core
    $ safe get secret/dc01/proto/shield/keys/core
    --- # secret/dc01/proto/shield/keys/core
    fingerprint: 40:9b:11:82:67:41:23:a8:c2:87:98:5d:ec:65:1d:30
    private: |
      -----BEGIN RSA PRIVATE KEY-----
      MIIEowIBAAKCAQEA+hXpB5lmNgzn4Oaus8nHmyUWUmQFmyF2pa1++2WBINTIraF9
      ... etc ...
      5lm7mGwOCUP8F1cdPmpPNCkoQ/dx3T5mnsCGsb3a7FVBDDBje1hs
      -----END RSA PRIVATE KEY-----
    public: |
      ssh-rsa AAAAB3NzaC...4vbnncAYZPTl4KOr

(output snipped for brevity and security; but mostly brevity)

Now we can put references to our Vaultified keypair in
``credentials.yml``:

::

    ---
    meta:
      shield_public_key: (( vault "secret/dc01/proto/shield/keys/core:public" ))

You may want to take this opportunity to migrate credentials-oriented
keys from ``properties.yml`` into this file.

Now, we should have only two errors left when we ``make manifest``:

::

    $ make manifest
    2 error(s) detected:
     - $.cloud_provider.properties.openstack.default_security_groups: What Security Groups?
     - $.networks.default.subnets: Specify subnets for your BOSH vm's network


    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

So it's down to networking.

Refer back to your `Network
Plan <https://github.com/starkandwayne/codex/blob/master/network.md>`__,
and find the ``global-infra-0`` subnet for the proto-BOSH in Horizon. If
you're using the plan in this repository, that would be ``10.4.1.0/24``,
and we're allocating ``10.4.1.0/28`` to our BOSH Director. Our
``networking.yml`` file, then, should look like this:

::

    ---
    networks:
      - name: default
        subnets:
          - range: 10.4.1.0/24
            gateway: 10.4.1.1
            dns: [10.4.1.77, 10.4.1.78]
            cloud_properties:
              net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   # <- Global-Infra-0 Network UUID
            reserved:
              - 10.4.1.2 - 10.4.1.3
              - 10.4.1.10 - 10.4.1.254
            static:
              - 10.4.1.4
    jobs:
      - name: bosh
        networks:
        - name: default
          static_ips: (( static_ips(0) ))

    cloud_provider:
      properties:
        openstack:
          default_security_groups: [default]

Our range is that of the actual subnet we are in, ``10.4.1.0/24`` (in
reality, the ``/28`` allocation is merely a tool of bookkeeping). As
such, our neutron-provided default gateway is 10.4.1.1 (the first
available IP from the associated router). DNS needs to be the IP's
provided by your OpenStack administrator.

We identify our OpenStack-specific configuration under
``cloud_properties`` by providing the **Network UUID**, NOT the subnet
UUID, of the internal neutron network we wish to use. We also define the
security groups BOSH will be bound to.

Under the ``reserved`` block, we reserve the first few IPs (in case they
are used for other network services such as DNS, etc.), and everything
outside of ``10.4.1.0/28`` (that is, ``10.4.1.16`` and above).

Finally, in ``static`` we reserve the first usable IP (``10.4.1.4``) as
static. This will be assigned to our ``bosh/0`` director VM.

Now, ``make manifest`` should succeed (no output is a good sign), and we
should have a full manifest at ``manifests/manifest.yml``:

::

    $ make manifest
    $ ls -l manifests/
    total 8
    -rw-r--r-- 1 ops staff 4572 Jun 28 14:24 manifest.yml

Now we are ready to deploy **proto-BOSH**.

::

    $ make deploy
    No existing genesis-created bosh-init statefile detected. Please
    help genesis find it.
    Path to existing bosh-init statefile (leave blank for new
    deployments):
    Deployment manifest: '~/ops/bosh-deployments/dc01/proto/manifests/.deploy.yml'
    Deployment state: '~/ops/bosh-deployments/dc01/proto/manifests/.deploy-state.json'

    Started validating
      Downloading release 'bosh'... Finished (00:00:09)
      Validating release 'bosh'... Finished (00:00:03)
      Downloading release 'bosh-openstack-cpi'... Finished (00:00:02)
      Validating release 'bosh-openstack-cpi'... Finished (00:00:00)
      Downloading release 'shield'... Finished (00:00:10)
      Validating release 'shield'... Finished (00:00:02)
      Validating cpi release... Finished (00:00:00)
      Validating deployment manifest... Finished (00:00:00)
      Downloading stemcell... Finished (00:00:01)
      Validating stemcell... Finished (00:00:00)
    Finished validating (00:00:29)
    ...

(At this point, ``bosh-init`` starts the tedious process of compiling
all the things. End-to-end, this is going to take about a half an hour,
so you probably want to go play `a game <http://slither.io>`__ or grab a
cup of tea.)

...

All done? Verify the deployment by trying to ``bosh target`` the
newly-deployed Director. First you're going to need to get the password
out of our **vault-init**.

::

    $ safe get secret/dc01/proto/bosh/users/admin
    --- # secret/dc01/proto/bosh/users/admin
    password: super-secret

Then, run target the director:

::

    $ bosh target https://10.4.1.4:25555 proto-bosh
    Target set to `dc01-proto-bosh'
    Your username: admin
    Enter password:
    Logged in as `admin'

    $ bosh status
    Config
                 ~/.bosh_config

    Director
      Name       dc01-proto-bosh
      URL        https://10.4.1.4:25555
      Version    1.3232.2.0 (00000000)
      User       admin
      UUID       a43bfe93-d916-4164-9f51-c411ee2110b2
      CPI        openstack_cpi
      dns        disabled
      compiled_package_cache disabled
      snapshots  disabled

    Deployment
      not set

All set!

Before you move onto the next step, you should commit your local
deployment files to version control, and push them up *somewhere*. It's
ok, thanks to Vault, Spruce and Genesis, there are no credentials or
anything sensitive in the template files.

Generate Vault Deploy
~~~~~~~~~~~~~~~~~~~~~

We're building the infrastructure environment's vault.

.. image:: /images/bastion_step_3.png
   :alt: Vault

Now that we have a **proto-BOSH** Director, we can use it to deploy our
real Vault. We'll start with the Genesis template for Vault:

::

    $ cd ~/ops
    $ genesis new deployment --template vault
    $ cd ~/ops/vault-deployments

**NOTE**: What is the "ops" environment? Short for operations, it's the
environment we're deploying the **proto-BOSH** and all the extra
software that monitors each of the child environments that will deployed
later by the **proto-BOSH** Director.

As before (and as will become almost second-nature soon), let's create
our ``dc01`` site using the ``openstack`` template, and then create the
``ops`` environment inside of that site.

::

    $ genesis new site --template openstack dc01
    $ genesis new env dc01 proto

Answer yes twice and then enter a name for your Vault instance when
prompted for a FQDN.

::

    $ cd ~/ops/vault-deployments/dc01/proto
    $ make manifest
    7 error(s) detected:
    - $.meta.openstack.azs.z1: Define the z1 OpenStack availability zone
    - $.meta.openstack.azs.z2: Define the z2 OpenStack availability zone
    - $.meta.openstack.azs.z3: Define the z3 OpenStack availability zone
    - $.networks.vault_z1.subnets: Specify the z1 network for vault
    - $.networks.vault_z2.subnets: Specify the z2 network for vault
    - $.networks.vault_z3.subnets: Specify the z3 network for vault
    - $.properties.vault.ha.domain: What fully-qualified domain name do you want to access your Vault at?


    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

Vault is pretty self-contained, and doesn't have any secrets of its own.
All you have to supply is your network configuration, and any IaaS
settings.

Referring back to our `Network
Plan <https://github.com/starkandwayne/codex/blob/master/network.md>`__
again, we find that Vault should be striped across three zone-isolated
networks:

-  **10.4.1.16/28** in zone 1 (a)
-  **10.4.2.16/28** in zone 2 (b)
-  **10.4.3.16/28** in zone 3 (c)

First, lets do our OpenStack-specific region/zone configuration, along
with our Vault HA fully-qualified domain name in ``properties.yml``:

::

    ---
    meta:
      openstack:
        azs:
          z1: dc01
          z2: dc01
          z3: dc01

    properties:
      vault:
        ha:
          domain: 10.4.1.17

Our ``/28`` ranges are actually in their corresponding ``/24`` ranges
because the ``/28``'s are (again) just for bookkeeping and ACL
simplification. That leaves us with this for our ``networking.yml``:

::

    ---
    networks:
      - name: vault_z1
        subnets:
        - range:    10.4.1.0/24
          gateway:  10.4.1.1
          dns:     [8.8.8.8, 8.8.4.4]
          cloud_properties:
            net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   #  ID for global-infra-0
            security_groups: [wide-open]
          reserved:
            - 10.4.1.2 - 10.4.1.15
            - 10.4.1.32 - 10.4.1.254                       # Vault (z1) is in 10.4.1.16/28
          static:
            - 10.4.1.16 - 10.4.1.18

      - name: vault_z2
        subnets:
        - range:    10.4.2.0/24
          gateway:  10.4.2.1
          dns:     [8.8.8.8, 8.8.4.4]
          cloud_properties:
            net_id: 2977ae9f-88f5-4d12-ad8e-1e393731ebb7   #  ID for global-infra-1
            security_groups: [wide-open]
          reserved:
            - 10.4.2.2 - 10.4.2.15
            - 10.4.2.32 - 10.4.2.254                       # Vault (z2) is in 10.4.2.16/28
          static:
            - 10.4.2.16 - 10.4.2.18

      - name: vault_z3
        subnets:
        - range:    10.4.3.0/24
          gateway:  10.4.3.1
          dns:     [8.8.8.8, 8.8.4.4]
          cloud_properties:
            net_id: 47f76643-ee72-44a3-b47f-a43e9c6ea8d2   #  ID for global-infra-2
            security_groups: [wide-open]
          reserved:
            - 10.4.3.2 - 10.4.3.15
            - 10.4.3.32 - 10.4.3.254                       # Vault (z3) is in 10.4.3.16/28
          static:
            - 10.4.3.16 - 10.4.3.18

That's a ton of configuration, but when you break it down it's not all
that bad. We're defining three separate networks (one for each of the
three availability zones). Each network has a unique OpenStack Network
UUID, but they share the same Security Groups, since we want uniform
access control across the board.

The most difficult part of this configuration is getting the reserved
ranges and static ranges correct, and self-consistent with the network
range / gateway / DNS settings. This is a bit easier since our network
plan allocates a different ``/24`` to each zone network, meaning that
only the third octet has to change from zone to zone (x.x.1.x for zone
1, x.x.2.x for zone 2, etc.)

Now, let's try a ``make manifest`` again (no output is a good sign):

::

    $ make manifest

And then let's give the deploy a whirl:

::

    $ make deploy
    Acting as user 'admin' on 'dc01-proto-bosh'
    Checking whether release consul/20 already exists...NO
    Using remote release `https://bosh.io/d/github.com/cloudfoundry-community/consul-boshrelease?v=20'

    Director task 1

Thanks to Genesis, we don't even have to upload the BOSH releases (or
stemcells) ourselves!

Initializing Your Global Vault
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Now that the Vault software is spinning, you're going to need to
initialize the Vault, which generates a root token for interacting with
the Vault, and a set of 5 *seal keys* that will be used to unseal the
Vault so that you can interact with it.

First off, we need to find the IP addresses of our Vault nodes:

::

    $ bosh vms dc01-proto-vault
    +---------------------------------------------------+---------+-----+----------+-----------+
    | VM                                                | State   | AZ  | VM Type  | IPs       |
    +---------------------------------------------------+---------+-----+----------+-----------+
    | vault_z1/0 (9fe19a85-e9ed-4bab-ac80-0d3034c5953c) | running | n/a | small_z1 | 10.4.1.16 |
    | vault_z2/0 (13a46946-cd06-46e5-8672-89c40fd62e5f) | running | n/a | small_z2 | 10.4.2.16 |
    | vault_z3/0 (3b234173-04d4-4bfb-b8bc-5966592549e9) | running | n/a | small_z3 | 10.4.3.16 |
    +---------------------------------------------------+---------+-----+----------+-----------+

(Your UUIDs may vary, but the IPs should be close.)

Let's target the vault at 10.4.1.16:

::

    $ export VAULT_ADDR=https://10.4.1.16:8200 && export VAULT_SKIP_VERIFY=1

We have to set ``$VAULT_SKIP_VERIFY`` to a non-empty value because we
used self-signed certificates when we deployed our Vault. The error
message is as following if we did not do ``export VAULT_SKIP_VERIFY=1``.

::

    !! Get https://10.4.1.16:8200/v1/secret?list=1: x509: cannot validate certificate for 10.4.1.16 because it doesn't contain any IP SANs

Ideally, in a production-ready environment you will not be using
self-signed certificates. In that case, you would skip this step.

Let's initialize the Vault:

::

    $ vault init
    Unseal Key 1: c146f038e3e6017807d2643fa46d03dde98a2a2070d0fceaef8217c350e973bb01
    Unseal Key 2: bae9c63fe2e137f41d1894d8f41a73fc768589ab1f210b1175967942e5e648bd02
    Unseal Key 3: 9fd330a62f754d904014e0551ac9c4e4e520bac42297f7480c3d651ad8516da703
    Unseal Key 4: 08e4416c82f935570d1ca8d1d289df93a6a1d77449289bac0fa9dc8d832c213904
    Unseal Key 5: 2ddeb7f54f6d4f335010dc5c3c5a688b3504e41b749e67f57602c0d5be9b042305
    Initial Root Token: e63da83f-c98a-064f-e4c0-cce3d2e77f97

    Vault initialized with 5 keys and a key threshold of 3. Please
    securely distribute the above keys. When the Vault is re-sealed,
    restarted, or stopped, you must provide at least 3 of these keys
    to unseal it again.

    Vault does not store the master key. Without at least 3 keys,
    your Vault will remain permanently sealed.

**Store these seal keys and the root token somewhere secure!!** (A
password manager like 1Password is an excellent option here.)

Unlike the dev-mode **vault-init** we spun up at the very outset, this
Vault comes up sealed, and needs to be unsealed using three of the five
keys above, so let's do that.

::

    $ vault unseal
    Key (will be hidden):
    Sealed: true
    Key Shares: 5
    Key Threshold: 3
    Unseal Progress: 1

    $ vault unseal
    ...

    $ vault unseal
    Key (will be hidden):
    Sealed: false
    Key Shares: 5
    Key Threshold: 3
    Unseal Progress: 0

Now, let's switch back to using ``safe``:

::

    $ safe target https://10.4.1.16:8200 proto
    Now targeting proto at https://10.4.1.16:8200

    $ safe auth token
    Authenticating against proto at https://10.4.1.16:8200
    Token:

    $ safe set secret/handshake knock=knock
    knock: knock

Migrating Credentials
~~~~~~~~~~~~~~~~~~~~~

You should now have two ``safe`` targets, one for first Vault (named
'init') and another for the real Vault (named 'proto'):

::

    $ safe targets

    (*) proto     https://10.4.1.16:8200
        init      http://127.0.0.1:8200

Our ``proto`` Vault should be empty; we can verify that with
``safe tree``:

::

    $ safe target proto -- tree
    Now targeting proto at https://10.4.1.16:8200
    .
    └── secret
        └── handshake

``safe`` supports a handy import/export feature that can be used to move
credentials securely between Vaults, without touching disk, which is
exactly what we need to migrate from our dev-Vault to our real one:

::

    $ safe target init -- export secret | \
      safe target proto -- import
    Now targeting proto at https://10.4.1.16:8200
    Now targeting init at http://127.0.0.1:8200
    wrote secret/dc01/proto/bosh/blobstore/director
    wrote secret/dc01/proto/bosh/db
    wrote secret/dc01/proto/bosh/vcap
    wrote secret/dc01/proto/vault/tls
    wrote secret/dc01
    wrote secret/dc01/proto/bosh/blobstore/agent
    wrote secret/dc01/proto/bosh/registry
    wrote secret/dc01/proto/bosh/users/admin
    wrote secret/dc01/proto/bosh/users/hm
    wrote secret/dc01/proto/shield/keys/core
    wrote secret/handshake
    wrote secret/dc01/proto/bosh/nats

    $ safe target proto -- tree
    Now targeting proto at https://10.4.1.16:8200
    .
    └── secret
        ├── handshake
        ├── dc01
        └── dc01/
            └── proto/
                ├── bosh/
                │   ├── blobstore/
                │   │   ├── agent
                │   │   └── director
                │   ├── db
                │   ├── nats
                │   ├── registry
                │   ├── users/
                │   │   ├── admin
                │   │   └── hm
                │   └── vcap
                ├── shield/
                │   └── keys/
                │       └── core
                └── vault/
                    └── tls

Voila! We now have all of our credentials in our real Vault, and we can
kill the **vault-init** server process!

::

    $ sudo pkill vault

**NOTE:** Since the ``init`` Vault is no longer valid, you may want to
delete its configuration out of your ``~/.saferc`` file by removing the
``init: http://127.0.0.1:8200`` line and its associated target.

SHIELD
------

.. image:: /images/bastion_step_4.png
   :alt: Shield

SHIELD is our backup solution. We use it to configure and schedule
regular backups of data systems that are important to our running
operation, like the BOSH database, Concourse, and Cloud Foundry.

Setting up Object Storage For Backup Archives
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To help keep things isolated, we're going to set up a brand new user
just for backup archive storage. It's a good idea to name this user
something like ``backup`` or ``shield-backup`` so that no one tries to
re-purpose it later, and so that it doesn't get deleted.

We also need to generate an S3 access key for this user and store those
credentials in the Vault:

::

    $ openstack ec2 credentials create --user shield-backup --project cf
    +------------+----------------------------------+
    | Field      | Value                            |
    +------------+----------------------------------+
    | access     | 453389616a724f74b5ba0c9e6874f77d |
    | project_id | 4116a3a098e64ff086b21ffba9dd2b2e |
    | secret     | 64206456a18946f88399103be7dc6a8f |
    | trust_id   | None                             |
    | user_id    | 95aaf239306f45759d0adc7f4855c12d |
    +------------+----------------------------------+

    $ export VAULT_PREFIX=secret/dc01/proto/shield
    $ safe set ${VAULT_PREFIX}/s3 access_key secret_key
    access_key [hidden]:
    access_key [confirm]:

    secret_key [hidden]:
    secret_key [confirm]:

You're also going to want to provision a dedicated bucket to store
archives in, and name it something descriptive, like ``codex-backups``.

Deploying SHIELD
~~~~~~~~~~~~~~~~

We'll start out with the Genesis template for SHIELD:

::

    $ cd ~/ops
    $ genesis init -k shield
    $ cd shield-deployments
    $ genesis new dc01-proto

This will prompt for parameters required to deploy shield such as authentication, static IP, etc.

Once finished, if OAuth Provider authentication was selected, open the dc01-proto.yml file that was
generated and under the "authentication" section fill out the mapping from oauth->tenants using the
example provided below.

::

    authentication:
    - name:       Github
      identifier: github
      backend:    github
      properties:
        client_id:     client-id-here
        client_secret: client-secret-here
        mapping:
          - github: starkandwayne  # <-- github org name
            tenant: starkandwayne  # <-- shield tenant name
            rights:
              - team: Owners       # <-- github team name
                role: admin        # <-- shield role name
              - team: Engineering  #   (first match wins)
                role: engineer
              - role: operator     # = (default match)

          - github: starkandwayne
            tenant: SYSTEM
            rights:
              - team: Engineering
                role: admin

          - github: cloudfoundry-community
            tenant: CF Community
            rights:
              - role: engineer
    - name:       UAA
      identifier: uaa1
      backend:    uaa
      properties:
        client_id:       shield-dev
        client_secret:   s.h.i.e.l.d.
        uaa_endpoint:    https://10.244.156.2:8443
        skip_verify_tls: true
        mapping:
          - tenant: UAA          # <-- shield tenant name
            rights:
              - scim: uaa.admin  # <-- uaa scim right
                role: admin      # <-- shield role
                                #   (first match wins)
              - scim: cloud_controller.write
                role: engineer
              - role: operator   # = (default match)
          - tenant: UAA Admins Club
            rights:
              - scim: uaa.admin
                role: admin

Time to deploy!

::

    $ genesis deploy dc01-proto
    Acting as user 'admin' on 'dc01-proto-bosh'
    Checking whether release shield/8.0.6 already exists...NO
    Using remote release `https://github.com/starkandwayne/shield-boshrelease/releases/download/v8.0.6/shield-8.0.6.tgz'

    Director task 13
      Started downloading remote release > Downloading remote release

Once that's complete, you will be able to access your SHIELD deployment,
and start configuring your backup jobs via the WebUI or CLI accessable
at the IP you specified for SHIELD.

**Deploying SHIELD Agents**

To deploy SHIELD a shield agent via  a new genesis deployment, simply 
answer ``yes`` to ``Do you want to install SHIELD on your <deployment> for backups?`` 
and answer the questions that follow. If you do not yet have a SHIELD deployed,
you would say ``no`` to the above question and use the method below to deploy SHIELD
agents.

If you are adding an agent to an existing genesis deployment, modify the
<environment>.yml file and add the following parameters.

::

    kit:
        subkits:
        - existing subkits
        - ...
        - shield
        - shield-<database-type> #<- this is only for CF with an internal DB

    params:
        #Existing Parameters
        ...

        # This is usually something like `secret/path/to/keys/for/shield/agent:public`
        # If you are unsure, use `safe tree` to find it.
        shield_key_vault_path: secret/path/to/keys/for/shield/agent:public

        # This is usually something like `secret/path/to/keys/for/shield/certs/ca:certificate`
        # If you are unsure, use `safe tree` to find it.
        shield_ca_vault_path: secret/path/to/keys/for/shield/certs/ca:certificate

        # This is usually something like "https://shield.example.com" or "https://xxx.xxx.xxx.xxx"
        shield_core_url: https://192.168.10.121

How to Use SHIELD
~~~~~~~~~~~~~~~~~

Backup jobs for SHIELD are created and maintained in the SHIELD UI:

.. image:: /images/shield_ui.png
   :alt: SHIELD UI

To access the SHIELD UI, go to https://shield-ip. The default user 
name is ``admin`` and the default password is ``shield``. We recommend
changing this passowrd and then also storing it in a password manager 
for convenience.

Upon first login, SHIELD with be uninitialized and require a master
password. This master password is used to unseal SHIELD and the
internal encryption key storage used to perform backups. Ensure this
password is saved in a password manager and/or vault as there is no
way to recover or reset this password if it is forgotten. The master
password can also be rotated under the Admin section of the WebUI or
via the CLI. Also, whenever SHIELD is redeployed or the SHIELD daemon
is restarted, it will come up in a ``Locked`` state and prompt admin
users for the master password. While in this state, backups will not
be scheduled and running backups or restores manually will fail until
SHIELD is unsealed. The current status of SHIELD is displayed in the HUD
at the top of the WebUI.

.. image:: /images/shield_fixed_key.png
   :alt: SHIELD Fixed Key

Upon entering the master password you will be directed to the above
screen. This is the SHIELD key used for fixed-key encrypted backups. To
backup SHIELD itself, you must use the fixed-key option to be able
to recover and decrypt the archive in the event of a disaster. This
is due to the fact that the internal encryption key storage is part 
of shield itself. Before leaving this screen you must save this key in
vault or a password manager as there is no way to recover this key
once you acknowledge. The key can be rotated when you rotate the master 
key, however the current key can not be recovered after navigating away
from this screen.

**Configuring A Job**

To configure SHIELD backup jobs, on the left hand sidebar select the
`Configure a new backup job` menu option. From here it will guide you
through a wizard to set up targets, schedules, retention policies, and
storage systems.

For this example we will backup SHIELD itself to Amazon S3. The name for
this target will be `SHIELD`, Notes can include the reasoning for using
fixed-key encryption or other target specifc things other operators may
need to know when editing the job in the future.

For the agent, we will select ``dc01-proto-shield/shield@z1/0 (at 192.168.10.121:5444)``.
This is the agent that resides on the SHIELD and will be used to backup
SHIELD itself. Generally an agent is installed on the same instance as
the service intended to be backed up. (More on this later)

The plugin used to backup SHIELD is `Local Filesystem Plugin (fs)`.
Once selected, the parameters specfic to the plugin will be displayed
below for plugin configuration. In this case, the Base Directory is
`/var/vcap/store/shield`, Files to Include/Exclude can remain blank,
and `Fixed-Key Encryption` must be checked for reasons stated above.
Once these parameters are filled in, click next.

Next, select a schedule for this jub to run. There are actually quite a 
few parameters available allowing you to create backups that are 
``hourly``, ``daily``, ``weekly``, or ``monthly``. Here are some 
additional backup schedules to show their behaviors: "every 4 hours from 12:15", 
"thursdays at 23:35", "3rd Tuesday at 2:05", and "monthly at 2:05 on 14th".
After selecting an appropriate schedule, click next.

Now that you have a backup schedule, we're going to create a **retention 
policy** as well. With retention policies it is helpful to include the 
duration in the policy name. The duration is given in days, so if you 
wanted to keep a given backup for a year you'd use ``365`` and perhaps 
name the policy "1 year retention".

Something to consider: people usually like comparing "this time, last
period" backups. By that we mean "I wonder what X looked like this time
last year" or "I wonder what last Monday looked like", so you might want
to consider making your 1 year backups actually 13 months or your weekly
backups 8 days. (And so on.)

Next up is configuring **sotrage systems** for the backup archives.
Currently SHIELD has s3, gcp, azure, swift, and webdav plugins for storage. 
For this example, we will use Amazon S3. Select a name for the storage system
to be used for configuring future jobs and managing the storage system. Add
any necessary notes and set a threshold for storage. This threshold will be
used to warn via the HUD and storage tab when total archive size on that system
reaches the threshold. For agent we will use the same one that we used to configured
that target. Plugin will be ``Amazon S3 Storage Plugin (s3)`` with the required
parameters being ``Access Key ID``, ``Secret Access Key``, and ``Bucket Name``.
While optional, it can be useful to use the `Bucket Path Prefix` parameter
to organize backups in the bucket if you plan on using this bucket for other
storage systems, services, etc. After filling out the parameters, click next.

At this point the new backup job is ready for review, notes, and a name.
If something is incorrect, simply click the part you wish to change and
reconfigure it to resolve the issue. Once satisfied with everything, click 
save to create the job.

**Other Notes**

In addition to running at the scheduled time, you can run a job at any
time by clicking the target on the systems page and then ``run now`` or
via ``Run an ad hoc backup`` on the left side menu. Jobs can
also be paused by using the SHIELD CLI with WebUI support coming soon. 
This means that the job will not run at its scheduled time(s) until it 
is unpaused.

In order to **restore** a given backup, select ``Restore data from a backup``
on the left side menu. You can filter your backup jobs and associated archives
by target name. The **Systems** page gives a list of the most recent tasks and their 
durations. Initially, most tasks are expected to have a very short duration
but as time goes on and your environment grows you will notice the time required 
for the various backups will increase.

Backing Up Other Services
~~~~~~~~~~~~~~~~~

This section will list the target configuration for other services. That said,
these parameters may vary from environment to environment and parameters may
have asterisks denoting notes below to clarify these cases. Also, parameters
denoted as ``blank`` can be left blank as they are optional in most cases,
however they can be configured if necessary.

**Bosh Director Backup**
::

    "Name": "Bosh"
    "Notes": "Bosh Director Backup"
    "Agent": "dc01-proto-bosh/bosh"
    "Backup Plugin": "PostgreSQL Backup Plugin (postgres)"
    "PostgreSQL Host": "127.0.0.1"
    "PostgreSQL Port": "5432"
    "PostgreSQL Username": "vcap"
    "PostgreSQL Password": ""*
    "Database to Backup": <blank>
    "Path to PostgreSQL bin/directory": "/var/vcap/packages/postgres/bin/"*
    "Fixed-Key Encryption?": <unchecked>


For ``PostgreSQL Password`` enter the literal two quote characters ``""``. This
is essentially an empty string as the vcap user does not require a password.

``Path to PostgreSQL bin/directory`` may vary from release to release. Verify
this via ssh to the bosh director and check the ``/var/vcap/packages`` folder
for the proper version of postgres and the bin directory.

**Concourse**
::

    "Name": "Concourse"
    "Notes": "Concourse Backup"
    "Agent": "dc01-proto-concourse/db"
    "Backup Plugin": "PostgreSQL Backup Plugin (postgres)"
    "PostgreSQL Host": "127.0.0.1"
    "PostgreSQL Port": "5432"
    "PostgreSQL Username": "vcap"
    "PostgreSQL Password": ""*
    "Database to Backup": <blank>
    "Path to PostgreSQL bin/directory": "/var/vcap/packages/postgres/bin/"*
    "Fixed-Key Encryption?": <unchecked>


For ``PostgreSQL Password`` enter the literal two quote characters ``""``. This
is essentially an empty string as the vcap user does not require a password.

``Path to PostgreSQL bin/directory`` may vary from release to release. Verify
this via ssh to the bosh director and check the ``/var/vcap/packages`` folder
for the proper version of postgres and the bin directory.

**Cloud Foundry Backup**
::

    "Name": "Cloud Foundry"
    "Notes": "Cloud Foundry Backup"
    "Agent": "dc01-proto-cf/db"
    "Backup Plugin": "PostgreSQL Backup Plugin (postgres)"
    "PostgreSQL Host": "127.0.0.1"
    "PostgreSQL Port": "5432"
    "PostgreSQL Username": "shield"
    "PostgreSQL Password": <found in vault>*
    "Database to Backup": <blank>
    "Path to PostgreSQL bin/directory": "/var/vcap/packages/postgres/bin/"*
    "Fixed-Key Encryption?": <unchecked>

The ``PostgreSQL Password`` can be found in vault under the cf tree in
the /postgres path. The key will be ``shield_password``. For example
``secret/dc01/proto/cf/postgres:shield_password`` or similar.

``Path to PostgreSQL bin/directory`` may vary from release to release. Verify
this via ssh to the bosh director and check the ``/var/vcap/packages`` folder
for the proper version of postgres and the bin directory.

**Vault**
::

    "Name": "Vault"
    "Notes": "Vault Backup"
    "Agent": "dc01-proto-vault/vault"
    "Backup Plugin": "Consul Backup Plugin (consul)"
    "Consul Host/Port": "https://127.0.0.1:8500"
    "Skip SSL Validation": <unchecked>
    "Consul Username": <blank>*
    "Consul Password": <blank>*
    "Fixed-Key Encryption?": <unchecked>


``Consul Username`` and ``Consul Password`` are blank in most Vault deployments
if this is not the case in your environment, update accordingly.

SHIELD currently has plugins for Redis, Mongo, Elasticsearch, and
others. To see more information about the plugin list and relevant
documentation, please check out the `SHIELD
README <https://github.com/starkandwayne/shield>`__.

bolo
----

.. image:: /images/bastion_step_5.png
   :alt: Bolo

Bolo is a monitoring system that collects metrics and state data from
your BOSH deployments, aggregates it, and provides data visualization
and notification primitives.

Deploying Bolo Monitoring
~~~~~~~~~~~~~~~~~~~~~~~~~

You may opt to deploy Bolo once for all of your environments, in which
case it belongs in your management network, or you may decide to deploy
per-environment Bolo installations. What you choose mostly only affects
your network topology / configuration.

To get started, you're going to need to create a Genesis deployments
repo for your Bolo deployments:

::

    $ cd ~/ops
    $ genesis new deployment --template bolo
    $ cd bolo-deployments

Next, we'll create a site for your datacenter or VPC. The bolo template
deployment offers some site templates to make getting things stood up
quick and easy, including:

-  ``aws`` for Amazon Web Services VPC deployments
-  ``openstack`` for OpenStack deployments
-  ``vsphere`` for VMWare ESXi virtualization clusters
-  ``bosh-lite`` for deploying and testing locally

::

    $ genesis new site --template openstack dc01
    Created site dc01 (from template openstack):
    ~/ops/bolo-deployments/dc01
    ├── README
    └── site
        ├── disk-pools.yml
        ├── jobs.yml
        ├── networks.yml
        ├── properties.yml
        ├── releases
        ├── resource-pools.yml
        ├── stemcell
        │   ├── name
        │   └── version
        └── update.yml

    2 directories, 10 files

Now, we can create our environment.

::

    $ cd ~/ops/bolo-deployments/dc01
    $ genesis new env dc01 proto
    Created environment dc01/proto:
    ~/ops/bolo-deployments/dc01/proto
    ├── Makefile
    ├── README
    ├── cloudfoundry.yml
    ├── credentials.yml
    ├── director.yml
    ├── monitoring.yml
    ├── name.yml
    ├── networking.yml
    ├── properties.yml
    └── scaling.yml

    0 directories, 10 files

Bolo deployments have no secrets, so there isn't much in the way of
environment hooks for setting up credentials.

Now let's make the manifest.

::

    $ cd ~/ops/bolo-deployments/dc01/proto
    $ make manifest

    2 error(s) detected:
     - $.meta.az: What availability zone is Bolo deployed to?
     - $.networks.bolo.subnets: Specify your bolo subnet

    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

From the error message, we need to configure the following things for an
OpenStack deployment of bolo:

-  Availability Zone (via ``meta.az``)
-  Networking configuration

According to the `Network
Plan <https://github.com/starkandwayne/codex/blob/master/network.md>`__,
the bolo deployment belongs in the **10.4.1.64/28** network, in dc01.
Let's configure the availability zone in ``properties.yml``:

::

    ---
    meta:
      az: dc01

Since ``10.4.1.64/28`` is subdivision of the ``10.4.1.0/24`` subnet, we
can configure networking as follows. Once again, we add a Floating IP so
we can access Gnossis.

::

    ---
    networks:
      - name: bolo
        subnets:
        - range: 10.4.1.0/24
          gateway: 10.4.1.1
          cloud_properties:
            net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   #  ID for global-infra-0
            security_groups: [wide-open]
          dns: [8.8.8.8, 8.8.4.4]
          reserved:
            - 10.4.1.2   - 10.4.1.3
            - 10.4.1.4 - 10.4.1.63
             # Bolo is in 10.4.1.64/28
            - 10.4.1.80 - 10.4.1.254
          static:
            - 10.4.1.65 - 10.4.1.68
      - name: floating
        type: vip
        cloud_properties:
          net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5
          security_groups: [wide-open]

    jobs:
    - name: bolo
      networks:
      - name: floating
        static_ips:
        - 192.168.10.114

You can validate your manifest by running ``make manifest`` and ensuring
that you get no errors (no output is a good sign).

Then, you can deploy to your BOSH Director via ``make deploy``.

Once you've deployed, you can validate the deployment via
``bosh deployments``. You should see the bolo deployment. You can find
the IP of bolo vm by running ``bosh vms`` for bolo deployment. In order
to visit the `Gnossis <https://github.com/bolo/gnossis>`__ web interface
on your ``bolo/0`` VM from your browser on your laptop, you need to
setup port forwarding to enable it.

One way of doing it is using ngrok, go to `ngrok
Downloads <https://ngrok.com/download>`__ page and download the right
version to your ``bolo/0`` VM, unzip it and run ``./ngrok http 80``, it
will output something like this:

::

    ngrok by @inconshreveable                                                                                                                                                                   (Ctrl+C to quit)

    Tunnel Status                 online
    Version                       2.1.3
    Region                        United States (us)
    Web Interface                 http://127.0.0.1:4040
    Forwarding                    http://18ce4bd7.ngrok.io -> localhost:80
    Forwarding                    https://18ce4bd7.ngrok.io -> localhost:80

    Connections                   ttl     opn     rt1     rt5     p50     p90
                                  0       0       0.00    0.00    0.00    0.00

Copy the http or https link for forwarding and paste it into your
browser, you will be able to visit the Gnossis web interface for bolo.

If you do not want to use ngrok, you can simply use your local built-in
SSH client as follows:

::

    ssh bastion -L 4040:<ip address of your bolo server>:80 -N

Then, go to http://127.0.0.1:4040 in your web browser.

Out of the box, the Bolo installation will begin monitoring itself for
general host health (the ``linux`` collector), so you should have graphs
for bolo itself.

Configuring Bolo Agents
~~~~~~~~~~~~~~~~~~~~~~~

Now that you have a Bolo installation, you're going to want to configure
your other deployments to use it. To do that, you'll need to add the
``bolo`` release to the deployment (if it isn't already there), add the
``dbolo`` template to all the jobs you want monitored, and configure
``dbolo`` to submit metrics to your ``bolo/0`` VM in the bolo
deployment.

**NOTE**: This may require configuration of network ACLs, security
groups, etc. If you experience issues with this step, you might want to
start looking in those areas first.

We will use shield as an example to show you how to configure Bolo
Agents.

To add the release:

::

    $ cd ~/ops/shield-deployments
    $ genesis add release bolo latest
    $ cd ~/ops/shield-deployments/dc01/proto
    $ genesis use release bolo

If you do a ``make refresh manifest`` at this point, you should see a
new release being added to the top-level ``releases`` list.

To configure dbolo, you're going to want to add a line like the last one
here to all of your job template definitions:

::

    jobs:
      - name: shield
        templates:
          - { release: bolo, name: dbolo }

Then, to configure ``dbolo`` to submit to your Bolo installation, add
the ``dbolo.submission.address`` property either globally or per-job
(strong recommendation for global, by the way).

If you have specific monitoring requirements, above and beyond the stock
host-health checks that the ``linux`` collector provides, you can change
per-job (or global) properties like the dbolo.collectors properties.

You can put those configuration in the ``properties.yml`` as follows:

::

    properties:
      dbolo:
        submission:
          address: x.x.x.x # your Bolo VM IP
        collectors:
          - { every: 20s, run: 'linux' }
          - { every: 20s, run: 'httpd' }
          - { every: 20s, run: 'process -n nginx -m nginx' }

Remember that you will need to supply the ``linux`` collector
configuration, since Bolo skips the automatic ``dbolo`` settings you get
for free when you specify your own configuration.

Further Reading on Bolo
~~~~~~~~~~~~~~~~~~~~~~~

More information can be found in the `Bolo BOSH Release
README <https://github.com/cloudfoundry-community/bolo-boshrelease>`__
which contains a wealth of information about available graphs,
collectors, and deployment properties.

Concourse
---------

.. image:: /images/bastion_step_6.png
   :alt: Concourse

Deploying Concourse
~~~~~~~~~~~~~~~~~~~

Make sure you are targeting the proto vault:

::

    $ safe target
    Currently targeting proto at https://10.4.1.16:8200

From the ``~/ops`` folder let's generate a new ``concourse`` deployment,
using the ``--template`` flag.

::

    $ genesis new deployment --template concourse

Inside the ``global`` deployment level goes the site level definition.
For this concourse setup we'll use an ``openstack`` template for an
``dc01`` site.

::

    $ genesis new site --template openstack dc01
    Created site dc01 (from template openstack):
    ~/ops/concourse-deployments/dc01
    ├── README
    └── site
        ├── disk-pools.yml
        ├── jobs.yml
        ├── networks.yml
        ├── properties.yml
        ├── releases
        ├── resource-pools.yml
        ├── stemcell
        │   ├── name
        │   └── version
        └── update.yml

    2 directories, 10 files

Finally now, because our vault is setup and targeted correctly we can
generate our ``environment`` level configurations. And begin the process
of setting up the specific parameters for our environment.

::

    $ cd ~/ops/concourse-deployments
    $ genesis new env dc01 proto
    Running env setup hook: ~/ops/concourse-deployments/.env_hooks/00_confirm_vault

    (*) proto   https://10.4.1.16:8200
        init    http://127.0.0.1:8200

    Use this Vault for storing deployment credentials?  [yes or no] yes
    Running env setup hook: ~/ops/concourse-deployments/.env_hooks/gen_creds
    Generating credentials for Concourse CI
    Created environment openstack/proto:
    ~/ops/concourse-deployments/dc01/proto
    ├── cloudfoundry.yml
    ├── credentials.yml
    ├── director.yml
    ├── Makefile
    ├── monitoring.yml
    ├── name.yml
    ├── networking.yml
    ├── properties.yml
    ├── README
    └── scaling.yml

Let's make the manifest:

::

    $ cd ~/ops/concourse-deployments/dc01/proto
    $ make manifest
    5 error(s) detected:
      - $.meta.availability_zone: What availability zone should your concourse VMs be in?
      - $.meta.external_url: What is the external URL for this concourse?
      - $.meta.shield_authorized_key: Specify the SSH public key from this environment's SHIELD daemon
      - $.meta.ssl_pem: Want ssl? define a pem
      - $.networks.concourse.subnets: Specify your concourse subnet

    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

Again starting with ``meta`` lines in
``~/ops/concourse-deployments/dc01/proto/properties.yml``:

::

    ---
    meta:
      availability_zone: "dc01"   # Set this to match your first zone
      external_url: "https://ci.192.168.10.115.sslip.io"  # Set as Floating IP address of the haproxy job
      ssl_pem: ~
      #  ssl_pem: (( vault meta.vault_prefix "/certs/haproxy:your_haproxy_domain" ))
      shield_authorized_key: (( vault "secret/dc01/proto/shield/keys/core:public" ))

The ``~`` means we won't use SSL certs for now. If you have proper certs
or want to use self signed you can add them to vault under the
``web_ui:pem`` key

For networking, we put this inside ``proto`` environment level (in
``networking.yml``):

::

    networks:
      - name: concourse
        subnets:
          - range: 10.4.1.0/24
            gateway: 10.4.1.1
            dns:     [8.8.8.8, 8.8.4.4]
            static:
              - 10.4.1.48 - 10.4.1.56  #Concourse uses 10.4.1.48/28
            reserved:
              - 10.4.1.2 - 10.4.1.3
              - 10.4.1.4 - 10.4.1.47
              - 10.4.1.65 - 10.4.1.254
            cloud_properties:
              net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266
              security_groups: [wide-open]

      - name: floating
        type: vip
        cloud_properties:
          net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5
          security_groups: [wide-open]

    jobs:
    - name: haproxy
      networks:
      - name: concourse
        default: [dns, gateway]
      - name: floating
        static_ips:
        - 192.168.10.115

After it is deployed, you can do a quick test by hitting the HAProxy
machine

::

    $ bosh vms dc01-proto-concourse
    Acting as user 'admin' on deployment 'dc01-proto-concourse' on 'dc01-proto-bosh'

    Director task 43

    Task 43 done

    +--------------------------------------------------+---------+-----+---------+------------+
    | VM                                               | State   | AZ  | VM Type | IPs        |
    +--------------------------------------------------+---------+-----+---------+------------+
    | db/0 (fdb7a556-e285-4cf0-8f35-e103b96eff46)      | running | n/a | db      | 10.4.1.61  |
    | haproxy/0 (5318df47-b138-44d7-b3a9-8a2a12833919) | running | n/a | haproxy | 10.4.1.51  |
    | web/0 (ecb71ebc-421d-4caa-86af-81985958578b)     | running | n/a | web     | 10.4.1.48  |
    | worker/0 (c2c081e0-c1ef-4c28-8c7d-ff589d05a1aa)  | running | n/a | workers | 10.4.1.62  |
    | worker/1 (12a4ae1f-02fc-4c3b-846b-ae232215c77c)  | running | n/a | workers | 10.4.1.57  |
    | worker/2 (b323f3ba-ebe4-4576-ab89-1bce3bc97e65)  | running | n/a | workers | 10.4.1.58  |
    +--------------------------------------------------+---------+-----+---------+------------+

    VMs total: 6

Verify you get a response from the ``haproxy`` IP address:

::

    $ curl -i 10.4.1.51
    HTTP/1.1 200 OK
    Date: Thu, 02 Jan 2017 04:50:05 GMT
    Content-Type: text/html; charset=utf-8
    Transfer-Encoding: chunked

    <!DOCTYPE html>
    <html lang="en">
      <head>
        <title>Concourse</title>

You can then run on a your local machine

::

    $ ssh -L 8080:10.4.1.51:80 user@ci.x.x.x.x.sslip.io -i path_to_your_private_key

and hit http://localhost:8080 to get the Concourse UI. Be sure to
replace ``user`` with the ``jumpbox`` username on the bastion host and
x.x.x.x with the IP address of the bastion host.

After the Concourse deployment is working, it is time to setup pipelines
for deployments. Once a single pipeline is set up, other pipelines are
set up using a similar workflow.

Run Genesis CI Basics
~~~~~~~~~~~~~~~~~~~~~

Genesis CI requires us to have at least three deployed environments:
alpha, beta and manual (/auto). Alpha is usually a bosh-lite
environment. In this model, once the changes are applied to the alpha
deployment and pass the tests, the changes will then be pushed further
down the pipeline automatically to the beta deployment, which is usually
in a sandbox/development environment. Once the tests pass the beta
environment, we can choose whether changes either manually or
automatically trigger your production deployment and tests. If manual,
then human intervention is required to trigger the production
deployment, by clicking the plus button in the Concourse UI. If
automatic, the pipeline itself will deploy to production environment
once beta deployment passes the tests.

In the deployments repo, run the following commands:

::

    genesis ci alpha site/env
    genesis ci beta  site/env
    genesis ci manual(or auto) site/env

Then simply run ``genesis ci check`` to see if everything is setup
correctly and use ``genesis ci flow`` to review that if the pipeline
flows are correct.

For the pipelines, you may recall in the beginning of the document that
the proto-BOSH is responsible for deploying SHIELD, Vault, etc. as well
as the other BOSHes. The other BOSHes are used to test deployment
upgrades for each environment until reaching the final environment. In
the current configuration upgrades are initially deployed to BOSH Lite
(``dc01-alpha-bosh-lite``) and then once they are passing are
automatically deployed to the staging (``dc01-staging-bosh``)
environment:

.. image:: /images/pipelines.png
   :alt: pipelines

Currently the pipelines are production-ready, meaning they only need the
director information if/when a production environment is added.
Typically, we recommend making production deployments manual to prevent
unintended and unscheduled changes to production.

For a manual deployment, simply click on ``dc01-prod`` and then the
``+`` in the upper right corner:

.. image:: /images/manual-deployment.png
   :alt: manual-deploy

By the necessity of its design, the BOSH pipeline differs from the other
pipelines. It will deploy to ``dc01-alpha-bosh-lite`` first and once
that passes it will use ``dc01-proto-openvdc`` to deploy
``dc01-staging-bosh``:

.. image:: /images/bosh-pipeline.png
   :alt: bosh-pipeline

Both ``dc01-alpha-bosh-lite`` and ``dc01-proto-openvdc`` will need to be
manually updated using either ``make manifest deploy`` or
``make refresh manifest deploy`` (``make refresh`` will update the
deployment with site and/or global changes that have been added).

Set Up Vault
~~~~~~~~~~~~

Create read-only policies
^^^^^^^^^^^^^^^^^^^^^^^^^

To limit the Vault access given to the Concourse workers, read-only
policies should be created for the pipelines. The number of policies
created depends on the required level of isolation. For lower security
development environments, one policy for all the pipelines which can
access all the credentials under ``secret/*`` can be configured. For
higher security production environments, one policy per pipeline can be
set up so each pipeline only has access to its own deployment secrets.
It is not good for the workers to authenticate with Vault using root,
since the root has full access to all secrets in all backends.

In order to create a read-only policy, run ``vault auth`` to log into
Vault as root. Run ``vault policies`` to show all the policies which are
already defined. In this example, we will only create one ``read-only``
policy for all of the pipelines. Run
``vault policy-write read-only acl.hcl`` to create a read-only policy
which has read-only rights to the ``secret/*`` path, where ``acl.hcl``
is configured as follows:

::

    path "secret/*" {
      policy = "read"
    }

To confirm, run ``vault policies`` to see the ``read-only`` policy we
created. Save the token for this policy and you can ``vault auth`` to
try it out later.

Set up AppID Authentication
^^^^^^^^^^^^^^^^^^^^^^^^^^^

NOTE: If the CLI to set up the app-id method does not work for you,
please check if you are using the right Vault CLI version.

Let’s take a look what type of auth methods are enabled:

::

    $ vault auth -methods
    Path    Type   Default TTL  Max TTL  Description
    token/  token  system       system   token based credentials

First, we need to enable the ``app-id`` auth method by running
``vault auth-enable app-id``.

::

    $ vault auth -methods
    Path     Type    Default TTL  Max TTL  Description
    app-id/  app-id  system       system
    token/   token   system       system   token based credentials

Next we need to configure an ``app-id`` token and ``user-id`` token, by
writing to the correct backend paths.

::

    vault write auth/app-id/map/app-id/your_app_id \
                  value=read-only\
                  display_name="Deployments pipeline"

    vault write auth/app-id/map/user-id/your_user_id \
            value=your_app_id \
            cidr_block= your_concourse_network_block

Keep in mind that ``genesis`` v1.6.0 has a default name for ``app-id``
and ``user-id`` for each deployment. Make sure you replace
``your_app_id`` and ``your_user_id`` using those default names
accordingly.

In future, we will switch to AppRole from AppId when ``genesis`` is
ready for AppRole. For more details, please visit:
https://www.vaultproject.io/docs/auth/app-id.html

Use FLY to Configure Pipelines
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The ``fly`` CLI is downloaded from the web UI page in your Concourse
environment its version that is in sync with the version of Concourse
that is deployed.

Set the fly target as ``concourse``. If we do not use ``concourse`` as
target name, we will need to specify it in the ``ci/settings.yml`` file.

``fly -t concourse login -c concourse_url``

You will be prompted for the user and password. The user name is
``concourse`` and the password is saved in
``secret/dc01/proto/concourse/web_ui`` in vault. If you do not specify a
team name, the CLI will log you into the default ``main`` team.

In this case, we are using Basic Auth. For details on how to set up
oAuth for your team, see `Authentication Management for
Teams <#authentication-management-for-teams>`__

To learn more about how to use the ``fly`` managing your pipelines,
click `here <https://concourse.ci/fly-cli.html>`__.

Configure and Generate Pipelines
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Run ``genesis ci repipe``. You will see the following errors:

::

    Testing Vault authentication by retrieving secret/handshake
    Key                     Value
    ---                     -----
    refresh_interval        2592000
    knock                   knock

    2 error(s) detected:
     - $.aliases.target: Please define aliases for your BOSH directors (uuid -> addr)
     - $.auth: Please define your BOSH directors in ci/boshes.yml (and remove this line)

    6 error(s) detected:
     - $.meta.github.owner: Please specify the name of the user / organization that owns the Github repository (in ci/settings.yml)
     - $.meta.github.private_key: Please generate an SSH Deployment Key for this repo and specify it in ci/settings.yml
     - $.meta.github.repo: Please specify the name of the Github repository (in ci/settings.yml)
     - $.meta.name: Please name this deployment pipeline (in ci/settings.yml)
     - $.meta.slack.channel: Please specify the channel (#name) or user (@user) to send messages to (in ci/settings.yml)
     - $.meta.slack.webhook: Please provide a Slack Integration WebHook (in ci/settings.yml)

We can tackle all the errors by configuring two files: ``ci/boshes.yml``
and ``ci/settings.yml``. Before that, lets add a deployment key to the
repo and write it to Vault, since we need it to configure our pipeline.

To generate an SSH key pair, use the following commands to write it to
Vault. In the git repo, add the public key to the deploy key.

::

    safe write secret/dc01/proto/concourse/deployment_keys "private_key_name@private_key_file"
    safe write secret/dc01/proto/concourse/deployment_keys "pub_key_name@pub_key_file"

Finally, configure the files we mentioned earlier:

::

    boshes.yml

    # The UUIDs and director URL's of all bosh directors in your pipeline go here.
    # Since we are using proto-bosh to deploy bosh-lite, dev-bosh and prod-bosh, using bosh-lite to deploy a regular bosh for pipeline alpha environment purpose, when you setup pipeline for BOSH, you will need configure for all the BOSHes which are involved.

    aliases:
      target:
        bosh_uuid: bosh_director_url
        bosh_uuid: bosh_director_url
        bosh_uuid: bosh_director_url

    # if you are setting up pipeline for BOSH, you also need to configure BOSH deployment names and URLs
        bosh_deployment_name: bosh_director_url
        bosh_deployment_name: bosh_director_url
        bosh_deployment_name: bosh_director_url

    auth:
      https://x.x.x.x:25555:
        username: admin
        password: (( vault "path to your bosh admin secret" ))
      https://x.x.x.x:25555:
        username: admin
        password: (( vault "path to your bosh admin secret" ))
      https://x.x.x.x:25555:
        username: admin
        password: (( vault "path to your bosh admin secret" ))

::

    settings.yml

    meta:
      name: your_pipeline_name
      env:
         VAULT_ADDR: YOUR_VAULT_ADDRESS
         VAULT_SKIP_VERIFY: 1

      github:
        owner: your_github_user_account
        repo: your_repo_name
        private_key: (( vault "your path to deploy key which you wrote to vault earlier" ))

      slack:
        webhook: (( vault "your path to webhook url" ))
        channel: '#your_channel name'

     # If you are setting up pipeline for BOSH, you need to configure pause_for_existing_bosh_tasks as true
      pause_for_existing_bosh_tasks: true

After ``genesis ci repipe`` succeeds, follow the instructions it prints
out to unpause your pipeline.

Note: We should never modify ``ci/pipeline.yml`` directly.
``genesis ci repipe`` will take what are in
``.ci.yml``,\ ``ci/settings.yml`` and ``ci/boshes.yml`` to generate
``ci/pipeline.yml``.

Adding Smoke Tests to Pipeline
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the deployment you set up pipeline for has a smoke tests errand, you
can add it to your existing pipeline pretty easily by following the
instruction below:

-  Run ``genesis ci smoke-test your_smoke_tests_errand_name``
-  Run ``genesis ci repipe``
-  Answer ``y`` when prompted to apply the configuration

Your pipeline configuration is now updated.

How to Use Concourse UI
~~~~~~~~~~~~~~~~~~~~~~~

Visit https://ci.192.168.10.115.sslip.io in your browser, and you will
see a "no pipelines configured" message in the middle of your screen.
Click the **login** button on the top right, choose the main team to
login. The username and password is the same with what you used when you
run ``fly -t concourse login -c concourse_url``. (If needed, you can
retrieve the password from vault with
``safe get secret/dc01/proto/concourse/web_ui:password``.) After you
login, you will see the pipelines listed on the left you already
configured as the main team. You can click the pipeline name to look at
the specific jobs in rectangle boxes of that pipeline.

Click each rectangle, you can see the builds, tasks and other details
about the corresponding job. To trigger a job manually, you can click
the plus button on the right corner for that job. We recommend that the
jobs which deploy to the production environment should be manually
triggered, and all other jobs can be triggered automatically when the
changes are pushed to the git repository.

Authentication Management for Teams
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For the main team, we use Github oAuth instead of Basic Auth. It is also
possible to set up authentication for additional teams. For more
information, please refer to the `Concourse Authentication
docs <https://concourse.ci/authentication.html>`__.

Building out Sites and Environments
-----------------------------------

Now that the underlying infrastructure has been deployed, we can start
deploying our alpha/beta/other sites, with Cloud Foundry, and any
required services. When using Concourse to update BOSH deployments,
there are the concepts of ``alpha`` and ``beta`` sites. The alpha site
is the initial place where all deployment changes are checked for sanity
+ deployability. Typically this is done with a ``bosh-lite`` VM. The
``beta`` sites are where site-level changes are vetted. Usually these
are referred to as the sandbox or staging environments, and there will
be one per site, by necessity. Once changes have passed both the alpha,
and beta site, we know it is reasonable for them to be rolled out to
other sites, like production.

Alpha
~~~~~

BOSH-Lite
^^^^^^^^^

Since our ``alpha`` site will be a bosh lite running on OpenStack, we
will need to deploy that to our `global infrastructure
network <https://github.com/starkandwayne/codex/blob/master/network.md>`__.

First, lets make sure we're in the right place, targeting the right
Vault:

::

    $ cd ~/ops
    $ safe target proto
    Now targeting proto at https://10.4.1.16:8200

Now we can create our repo for deploying the bosh-lite:

::

    $ genesis new deployment --template bosh-lite
    cloning from template https://github.com/starkandwayne/bosh-lite-deployment
    Cloning into '~/ops/bosh-lite-deployments'...
    remote: Counting objects: 55, done.
    remote: Compressing objects: 100% (33/33), done.
    remote: Total 55 (delta 7), reused 55 (delta 7), pack-reused 0
    Unpacking objects: 100% (55/55), done.
    Checking connectivity... done.
    Embedding genesis script into repository
    genesis v1.5.2 (ec9c868f8e62)
    [master 5421665] Initial clone of templated bosh-lite deployment
     3 files changed, 3672 insertions(+), 67 deletions(-)
      rewrite README.md (96%)
       create mode 100755 bin/genesis

Next lets create our site and environment:

::

    $ cd bosh-lite-deployments
    $ genesis new site --template openstack dc01
    Created site dc01 (from template openstack):
    ~/ops/bosh-lite-deployments/dc01
    ├── README
    └── site
        ├── disk-pools.yml
        ├── jobs.yml
        ├── networks.yml
        ├── properties.yml
        ├── README
        ├── releases
        ├── resource-pools.yml
        ├── stemcell
        │   ├── name
        │   └── version
        └── update.yml

    2 directories, 11 files

    $ genesis new env dc01 alpha
    Running env setup hook: ~/ops/bosh-lite-deployments/.env_hooks/setup

    (*) proto   https://10.4.1.16:8200

    Use this Vault for storing deployment credentials?  [yes or no]yes
    Setting up credentials in vault, under secret/dc01/alpha/bosh-lite
    .
    └── secret/dc01/alpha/bosh-lite
        ├── blobstore/


        │   ├── agent
        │   └── director
        ├── db
        ├── nats
        ├── users/
        │   ├── admin
        │   └── hm
        └── vcap




    Created environment dc01/alpha:
    ~/ops/bosh-lite-deployments/dc01/alpha
    ├── cloudfoundry.yml
    ├── credentials.yml
    ├── director.yml
    ├── Makefile


    ├── monitoring.yml
    ├── name.yml
    ├── networking.yml
    ├── properties.yml
    ├── README
    └── scaling.yml

    0 directories, 10 files

Now lets try to deploy:

::

    $ cd dc01/alpha/
    $ make deploy
      checking https://genesis.starkandwayne.com for details on latest stemcell bosh-openstack-kvm-ubuntu-trusty-go_agent
      checking https://genesis.starkandwayne.com for details on release bosh/260
      checking https://genesis.starkandwayne.com for details on release bosh-warden-cpi/29
      checking https://genesis.starkandwayne.com for details on release garden-linux/0.342.0
      checking https://genesis.starkandwayne.com for details on release port-forwarding/6
      3 error(s) detected:
       - $.meta.openstack.azs.z1: What Availability Zone will BOSH be in?
       - $.meta.port_forwarding_rules: Define any port forwarding rules you wish to enable on the bosh-lite, or an empty array
       - $.networks.default.subnets: Specify subnets for your BOSH vm's network


    Failed to merge templates; bailing...


    Makefile:25: recipe for target 'deploy' failed
    make: *** [deploy] Error 3

Looks like we only have a handful of parameters to update, all related
to networking, so lets fill out our ``networking.yml``, after consulting
the `Network
Plan <https://github.com/starkandwayne/codex/blob/master/network.md>`__
to find our global infrastructure network and Horizon to find our
Network UUID:

``networking.yml``

::

    ---
    networks:
    - name: default
      subnets:
      - cloud_properties:
          net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   #  ID for global-infra-0
          security_groups: [wide-open]
        dns:     [8.8.8.8]
        gateway: 10.4.1.1
        range:   10.4.1.0/24

Since there are a bunch of other deployments on the infrastructure
network, we should take care to reserve the correct static + reserved
IPs, so that we don't conflict with other deployments. Fortunately that
data can be referenced in the `Global Infrastructure IP Allocation
section <https://github.com/starkandwayne/codex/blob/master/part3/network.md#global-infrastructure-ip-allocation>`__
of the Network Plan:

``networking.yml``

::

    ---
    networks:
    - name: default
      subnets:
      - cloud_properties:
          net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   #  ID for global-infra-0
          security_groups: [wide-open]
        dns:     [8.8.8.8]
        gateway: 10.4.1.1
        range:   10.4.1.0/24
        reserved:
          - 10.4.1.2 - 10.4.1.79
          - 10.4.1.96 - 10.4.1.255
        static:
          - 10.4.1.80

As before, we will add the Floating IP so we can access Cloud Foundry
when it is deployed to bosh-lite:

::

    networks:
    - name: default
      subnets:
      - cloud_properties:
          net_id: b5bfe2d1-fa17-41cc-9928-89013c27e266   #  ID for global-infra-0
          security_groups: [wide-open]
        dns:     [8.8.8.8]
        gateway: 10.4.1.1
        range:   10.4.1.0/24
        reserved:
          - 10.4.1.2 - 10.4.1.79
          - 10.4.1.96 - 10.4.1.255
        static:
          - 10.4.1.80
    - name: floating
      type: vip
      cloud_properties:
        net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5
        security_groups: [wide-open]
    jobs:
    - name: bosh
      networks:
      - name: default
        default: [gateway, dns]
      - name: floating
        static_ips:
        - 192.168.10.124

Lastly, we will need to add port-forwarding rules, so that things
outside the bosh-lite can talk to its services. Since we know we will be
deploying Cloud Foundry, let's add rules for it:

``properties.yml``

::

    ---
    meta:
      openstack:
        azs:
          z1: dc01
      port_forwarding_rules:
        - internal_ip:   10.244.0.34
          internal_port: 80
          external_port: 80
        - internal_ip:   10.244.0.34
          internal_port: 443
          external_port: 443

And finally, we can deploy again:

::

    $ make deploy
      checking https://genesis.starkandwayne.com for details on stemcell bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3262.2
        checking https://genesis.starkandwayne.com for details on release bosh/256.2
      checking https://genesis.starkandwayne.com for details on release bosh-warden-cpi/29
        checking https://genesis.starkandwayne.com for details on release garden-linux/0.339.0
      checking https://genesis.starkandwayne.com for details on release port-forwarding/2
        checking https://genesis.starkandwayne.com for details on stemcell bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3262.2
      checking https://genesis.starkandwayne.com for details on release bosh/256.2
        checking https://genesis.starkandwayne.com for details on release bosh-warden-cpi/29
      checking https://genesis.starkandwayne.com for details on release garden-linux/0.339.0
        checking https://genesis.starkandwayne.com for details on release port-forwarding/2
    Acting as user 'admin' on 'dc01-proto-bosh'
    Checking whether release bosh/256.2 already exists...YES
    Acting as user 'admin' on 'dc01-proto-bosh'
    Checking whether release bosh-warden-cpi/29 already exists...YES
    Acting as user 'admin' on 'dc01-proto-bosh'
    Checking whether release garden-linux/0.339.0 already exists...YES
    Acting as user 'admin' on 'dc01-proto-bosh'
    Checking whether release port-forwarding/2 already exists...YES
    Acting as user 'admin' on 'dc01-proto-bosh'
    Checking if stemcell already exists...
    Yes
    Acting as user 'admin' on deployment 'dc01-alpha-bosh-lite' on 'dc01-proto-bosh'
    Getting deployment properties from director...
    Unable to get properties list from director, trying without it...

    Detecting deployment changes
    ...
    Deploying
    ---------
    Are you sure you want to deploy? (type 'yes' to continue): yes

    Director task 58
      Started preparing deployment > Preparing deployment. Done (00:00:00)
    ...
    Task 58 done

    Started     2017-01-02 19:14:31 UTC
    Finished    2017-01-02 19:17:42 UTC
    Duration    00:03:11

    Deployed `dc01-alpha-bosh-lite' to `dc01-proto-bosh'

Now we can verify the deployment and set up our ``bosh`` CLI target:

::

    # grab the admin password for the bosh-lite
    $ safe get secret/dc01/alpha/bosh-lite/users/admin
    --- # secret/dc01/alpha/bosh-lite/users/admin
    password: YOUR-PASSWORD-WILL-BE-HERE


    $ bosh target https://10.4.1.80:25555 alpha
    Target set to `dc01-alpha-bosh-lite'
    Your username: admin
    Enter password:
    Logged in as `admin'
    $ bosh status
    Config
                 ~/.bosh_config

     Director
       Name       dc01-alpha-bosh-lite
         URL        https://10.4.1.80:25555
       Version    1.3232.2.0 (00000000)
         User       admin
       UUID       d0a12392-f1df-4394-99d1-2c6ce376f821
         CPI        vsphere_cpi
       dns        disabled
         compiled_package_cache disabled
       snapshots  disabled

       Deployment
         not set

Tadaaa! Time to commit all the changes to deployment repo, and push to
where we're storing them long-term.

Alpha Cloud Foundry
^^^^^^^^^^^^^^^^^^^

To deploy CF to our alpha environment, we will need to first ensure
we're targeting the right Vault/BOSH:

::

    $ cd ~/ops
    $ safe target proto

    (*) proto   https://10.4.1.16:8200

    $ bosh target alpha
    Target set to `dc01-alpha-bosh-lite'

Now we'll create our deployment repo for cloudfoundry:

::

    $ genesis new deployment --template cf
    cloning from template https://github.com/starkandwayne/cf-deployment
    Cloning into '~/ops/cf-deployments'...
    remote: Counting objects: 268, done.
    remote: Compressing objects: 100% (3/3), done.
    remote: Total 268 (delta 0), reused 0 (delta 0), pack-reused 265
    Receiving objects: 100% (268/268), 51.57 KiB | 0 bytes/s, done.
    Resolving deltas: 100% (112/112), done.
    Checking connectivity... done.
    Embedding genesis script into repository
    genesis v1.5.2 (ec9c868f8e62)
    [master 1f0c534] Initial clone of templated cf deployment
     2 files changed, 3666 insertions(+), 150 deletions(-)
     rewrite README.md (99%)
     create mode 100755 bin/genesis

And generate our bosh-lite based alpha environment:

::

    $ cd cf-deployments
    $ genesis new site --template bosh-lite bosh-lite
    Created site bosh-lite (from template bosh-lite):
    ~/ops/cf-deployments/bosh-lite
    ├── README
    └── site
        ├── disk-pools.yml
        ├── jobs.yml
        ├── networks.yml
        ├── properties.yml
        ├── releases
        ├── resource-pools.yml
        ├── stemcell
        │   ├── name
        │   └── version
        └── update.yml

    2 directories, 10 files

    $ genesis new env bosh-lite alpha
    Running env setup hook: ~/ops/cf-deployments/.env_hooks/00_confirm_vault

    (*) proto   https://10.4.1.16:8200

    Use this Vault for storing deployment credentials?  [yes or no] yes
    Running env setup hook: ~/ops/cf-deployments/.env_hooks/setup_certs
    Generating Cloud Foundry internal certs
    Uploading Cloud Foundry internal certs to Vault
    Running env setup hook: ~/ops/cf-deployments/.env_hooks/setup_cf_secrets
    Creating JWT Signing Key
    Creating app_ssh host key fingerprint
    Generating secrets
    Created environment bosh-lite/alpha:
    ~/ops/cf-deployments/bosh-lite/alpha
    ├── cloudfoundry.yml
    ├── credentials.yml
    ├── director.yml
    ├── Makefile
    ├── monitoring.yml
    ├── name.yml
    ├── networking.yml
    ├── properties.yml
    ├── README
    └── scaling.yml

    0 directories, 10 files

Unlike all the other deployments so far, we won't use ``make manifest``
to vet the manifest for CF. This is because the bosh-lite CF comes out
of the box ready to deploy to a Vagrant-based bosh-lite with no tweaks.
Since we are using it as the Cloud Foundry for our alpha environment, we
will need to customize the Cloud Foundry base domain, with a domain
resolving to the IP of our ``alpha`` bosh-lite VM:

::

    cd bosh-lite/alpha

In ``properties.yml``:

::

    ---
    meta:
      cf:
        base_domain: 10.4.1.80.sslip.io

Now we can deploy:

::

    $ make deploy
      checking https://genesis.starkandwayne.com for details on release cf/250
      checking https://genesis.starkandwayne.com for details on release toolbelt/3.3.0
      checking https://genesis.starkandwayne.com for details on release postgres/1.0.3
      ...
    Acting as user 'admin' on 'dc01-try-anything-bosh-lite'
    Checking whether release cf/250 already exists...NO
    Using remote release `https://bosh.io/d/github.com/cloudfoundry/cf-release?v=250'

    Director task 1
      Started downloading remote release > Downloading remote release
    ...
    Deploying
    ---------
    Are you sure you want to deploy? (type 'yes' to continue): yes

    Director task 12
      Started preparing deployment > Preparing deployment. Done (00:00:01)
    ...
    Task 12 done

    Started     2017-01-02 14:47:45 UTC
    Finished    2017-01-02 14:51:28 UTC
    Duration    00:03:43

    Deployed `bosh-lite-alpha-cf' to `dc01-try-anything-bosh-lite'

And once complete, run the smoke tests for good measure:

::

    $ genesis bosh run errand smoke_tests
    Acting as user 'admin' on deployment 'bosh-lite-alpha-cf' on 'dc01-alpha-bosh-lite'

    Director task 18
      Started preparing deployment > Preparing deployment. Done (00:00:02)

      Started preparing package compilation > Finding packages to compile. Done (00:00:01)

      Started creating missing vms > smoke_tests/0 (c609e4c5-29e7-4f66-81e1-b94b9139ee7d). Done (00:00:08)

      Started updating job smoke_tests > smoke_tests/0 (c609e4c5-29e7-4f66-81e1-b94b9139ee7d) (canary). Done (00:00:23)

      Started running errand > smoke_tests/0. Done (00:02:18)

      Started fetching logs for smoke_tests/c609e4c5-29e7-4f66-81e1-b94b9139ee7d (0) > Finding and packing log files. Done (00:00:01)

      Started deleting errand instances smoke_tests > smoke_tests/0 (c609e4c5-29e7-4f66-81e1-b94b9139ee7d). Done (00:00:03)

    Task 18 done

    Started         2017-01-02 14:15:16 UTC
    Finished        2017-01-02 14:18:12 UTC
    Duration        00:02:56

    [stdout]
    ################################################################################################################
    go version go1.6.3 linux/amd64
    CONFIG=/var/vcap/jobs/smoke-tests/bin/config.json
    ...

    Errand 'smoke_tests' completed successfully (exit code 0)

We now have our alpha-environment's Cloud Foundry stood up!

Alpha haproxy
^^^^^^^^^^^^^

Haproxy primarily provides SSL termination for Cloud Foundry. It is also
configured to provide SSL termination for S3 since the current Openstack
environment doesn't have an SSL endpoint for S3, which is needed for
SHIELD. This will not be needed for environments that have SSL endpoints
for S3, e.g. production.

Since haproxy has a cert and credentials that need to go in Vault, make
sure you are targeting the desired Vault:

::

    $ safe target proto

Once that is taken care of, you will create the new deployment with its
site and environment:

::

    $ genesis new deployment --template haproxy
    $ cd haproxy-deployments
    $ genesis new site --template bosh-lite bosh-lite
    $ genesis new env bosh-lite alpha
    $ cd bosh-lite/alpha
    $ make manifest
    Found stemcell bosh-warden-boshlite-ubuntu-trusty-go_agent 3312.15 on director
    Found release haproxy latest on director
    release toolbelt track is set to track from the index
      checking https://genesis.starkandwayne.com for details on latest release toolbelt
    2 error(s) detected:
     - $.properties.ha_proxy.backend_servers: Specify your go routers IPs as backend servers
     - $.properties.ha_proxy.tcp.cf_app_ssh.backend_servers: Specify you  CF Access VMs IPs as your backend servers


    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

As before, we'll resolve the errors by adding in the requested
information, this time in the ``properties.yml`` file:

::

    ---
    properties:
      ha_proxy:
        backend_servers: [ 10.244.0.22 ]       # Alpha Cloud Foundry router_z1 IP
        ssl_pem:
          - (( vault meta.vault_prefix "/certs/pems:*.192.168.10.124.sslip.io" ))

        tcp:
          - name: cf_app_ssh
            backend_servers: [ 10.244.0.109 ]  # Alpha Cloud Foundry access_z1 IP

You'll notice that we also added another field, ``ssl_pem``. This stores
the SSL cert that will be used by haproxy. In beta environments if you
are using an ``sslip.io`` domain you will need to supply a floating IP
to generate the cert, but since this is BOSH Lite we will simply supply
the static IP assigned to the haproxy instance. To generate the cert,
run the ``haproxy_cert_gen`` script in the ``bin`` directory:

::

    $ ENV_PATH=secret/bosh-lite/alpha FIP=192.168.10.124 ./bin/haproxy_cert_gen

The script uses Cloud Foundry's CA, so the ``ENV_PATH`` supplied is the
``vault_prefix`` given in ``name.yml`` without the actual deployment
name.

If you are not using an ``sslip.io`` domain and are using a domain with
its own CA, you will not need to run this script. Rather you would use
your own internal process to generate the certs and then add them to
Vault with ``safe write``.

Once this is done all the errors are resolved and you can run
``make manifest deploy`` to deploy haproxy.

First Beta Environment
~~~~~~~~~~~~~~~~~~~~~~

Now that our ``alpha`` environment has been deployed, we can deploy our
first beta environment to OpenStack. To do this, we will first deploy a
BOSH Director for the environment using the ``bosh-deployments`` repo we
generated back when we built our `proto-BOSH <#proto-bosh>`__, and then
deploy Cloud Foundry on top of it.

BOSH
^^^^

::

    $ cd ~/ops/bosh-deployments
    $ bosh target proto-bosh
    $ ls
    dc01  bin  global  LICENSE  README.md

We already have the ``dc01`` site created, so now we will just need to
create our new environment, and deploy it. Different names (sandbox or
staging) for Beta have been used for different customers, here we call
it staging.

::

    $ safe target proto
    Now targeting proto at http://10.10.10.6:8200
    $ genesis new env dc01 staging
    RSA 1024 bit CA certificates are loaded due to old openssl compatibility
    Running env setup hook: ~/ops/bosh-deployments/.env_hooks/setup

     proto  http://10.10.10.6:8200

    Use this Vault for storing deployment credentials?  [yes or no] yes
    Setting up credentials in vault, under secret/dc01/staging/bosh
    .
    └── secret/dc01/staging/bosh
        ├── blobstore/
        │   ├── agent
        │   └── director
        ├── db
        ├── nats
        ├── users/
        │   ├── admin
        │   └── hm
        └── vcap


    Created environment dc01/staging:
    ~/ops/bosh-deployments/dc01/staging
    ├── cloudfoundry.yml
    ├── credentials.yml
    ├── director.yml
    ├── Makefile
    ├── monitoring.yml
    ├── name.yml
    ├── networking.yml
    ├── properties.yml
    ├── README
    └── scaling.yml

    0 directories, 10 files

Notice, unlike the **proto-BOSH** setup, we do not specify
``--type bosh-init``. This means we will use BOSH itself (in this case
the **proto-BOSH**) to deploy our sandbox BOSH. Again, the environment
hook created all of our credentials for us, but this time we targeted
the long-term Vault, so there will be no need for migrating credentials
around.

Let's try to deploy now, and see what information still needs to be
resolved:

::

    $ make manifest

    9 error(s) detected:
     - $.cloud_provider.properties.openstack.default_key_name: What is your full key name?
     - $.cloud_provider.properties.openstack.default_security_groups: What Security Groups?
     - $.cloud_provider.ssh_tunnel.private_key: What is the local path to the Private Key for this deployment?  Due to a bug in Openstack Liberty and Mitaka, you need to use an SSH key generated by ssh-keygen, not one generated by Nova.
     - $.meta.openstack.api_key: Please supply an Openstack password
     - $.meta.openstack.auth_url: Please supply the authentication URL for the Openstack Identity Service
     - $.meta.openstack.tenant: Please supply an Openstack tenant name
     - $.meta.openstack.username: Please supply an Openstack user name
     - $.meta.shield_public_key: Specify the SSH public key from this environment's SHIELD daemon
     - $.networks.default.subnets: Specify subnets for your BOSH vm's network


    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

Looks like we need to provide the same type of data as we did for
**proto-BOSH**. Lets fill in the basic properties:

::

    `properties.yml`

    ---
    meta:
      openstack:
        api_key:  (( vault meta.vault_prefix "/openstack:api_key" ))
        tenant:   (( vault meta.vault_prefix "/openstack:tenant" ))
        username: (( vault meta.vault_prefix "/openstack:username" ))
        auth_url: http://identity.mydatacenter.io:5000/v2.0
        region: os-dc1
    cloud_provider:
      properties:
        openstack:
          default_key_name: bosh
          connection_options:
            connect_timeout: 600
          ignore_server_availability_zone: true
      ssh_tunnel:
        host: (( grab jobs.bosh.networks.default.static_ips.0 ))
        private_key: ~/.ssh/bosh
    properties:
      bolo:
        submission:
          address: 10.4.1.65
        collectors:
          - { every: 20s, run: 'linux' }
    EOF

This was a bit easier than it was for **proto-BOSH** since our keys are
already in Vault.

Verifying our changes worked, we see that we only need to provide
networking configuration at this point:

::

    make deploy
    $ make deploy
    1 error(s) detected:
     - $.networks.default.subnets: Specify subnets for your BOSH vm's network


    Failed to merge templates; bailing...
    make: *** [deploy] Error 3

All that remains is filling in our networking details, so lets go
consult our `Network
Plan <https://github.com/starkandwayne/codex/blob/master/network.md>`__.
We will place the BOSH Director in the staging site's infrastructure
network, in the first AZ we have defined (subnet name
``staging-infra-0``, CIDR ``10.4.32.0/24``). To do that, we'll need to
update ``networking.yml``:

``networking.yml``:

::

    ---
    networks:
      - name: default
        subnets:
          - range: 10.4.16.0/24
            gateway: 10.4.16.1
            dns: [10.4.1.77, 10.4.1.78]
            cloud_properties:
              net_id: 20c35573-3a0c-4725-95a2-b58550407fcf # <- Global-Infra-0 Network UUID
            reserved:
              - 10.4.16.2 - 10.4.16.3
              - 10.4.16.10 - 10.4.16.254
            static:
              - 10.4.16.4
      - name: floating
        type: vip
        cloud_properties:
          net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5
          security_groups: [wide-open]

    jobs:
      - name: bosh
        networks:
        - name: default
          static_ips: (( static_ips(0) ))
        - name: floating
          static_ips:
          - 192.168.10.122

    cloud_provider:
      properties:
        openstack:
          default_security_groups: [default]
    EOF

Now that that's handled, let's deploy for real:

::

    $ make deploy
    RSA 1024 bit CA certificates are loaded due to old openssl compatibility
    Acting as user 'admin' on 'openstack-proto-bosh-microboshen-openstack'
    Checking whether release bosh/256.2 already exists...YES
    Acting as user 'admin' on 'openstack-proto-bosh-microboshen-openstack'
    Checking whether release bosh-openstack-cpi/53 already exists...YES
    Acting as user 'admin' on 'openstack-proto-bosh-microboshen-openstack'
    Checking whether release shield/6.2.1 already exists...YES
    Acting as user 'admin' on 'openstack-proto-bosh-microboshen-openstack'
    Checking if stemcell already exists...
    Yes
    Acting as user 'admin' on deployment 'dc01-staging-bosh' on 'openstack-proto-bosh-microboshen-openstack'
    Getting deployment properties from director...

    Detecting deployment changes
    ----------------------------
    resource_pools:
    - cloud_properties:
        availability_zone: us-east-1b
        ephemeral_disk:
          size: 25000
          type: gp2
        instance_type: m3.xlarge
      env:
        bosh:
          password: "<redacted>"
      name: bosh
      network: default
      stemcell:
        name: bosh-aws-xen-hvm-ubuntu-trusty-go_agent
        sha1: 971e869bd825eb0a7bee36a02fe2f61e930aaf29
        url: https://bosh.io/d/stemcells/bosh-aws-xen-hvm-ubuntu-trusty-go_agent?v=3232.6
    ...
    Deploying
    ---------
    Are you sure you want to deploy? (type 'yes' to continue): yes

    Director task 144
      Started preparing deployment > Preparing deployment. Done (00:00:00)

      Started preparing package compilation > Finding packages to compile. Done (00:00:00)
    ...
    Task 144 done

    Started     2017-01-02 17:23:47 UTC
    Finished    2017-01-02 17:34:46 UTC
    Duration    00:10:59

    Deployed 'dc01-staging-bosh' to 'dc01-proto-bosh'

This will take a little less time than **proto-BOSH** did (some packages
were already compiled), and the next time you deploy, it go by much
quicker, as all the packages should have been compiled by now (unless
upgrading BOSH or the stemcell).

Once the deployment finishes, target the new BOSH Director to verify it
works:

::

    $ safe get secret/dc01/staging/bosh/users/admin # grab the admin user's password for bosh
    $ bosh target https://10.4.32.4:25555 dc01-staging
    Target set to 'dc01-staging-bosh'
    Your username: admin
    Enter password:
    Logged in as 'admin'

Again, since our creds are already in the long-term vault, we can skip
the credential migration that was done in the proto-bosh deployment and
go straight to committing our new deployment to the repo, and pushing it
upstream.

Next, we will deploy our jumpbox.

Beta Jumpbox
^^^^^^^^^^^^

Unlike the proto jumpbox, which was initially deployed with Terraform,
the beta (dev) jumpbox can be deployed via BOSH:

::

    $ cd ~/ops/cf-deployments
    $ genesis new deployment --template jumpbox
    $ cd jumpbox-deployments
    $ genesis new site --template openstack dc01
    $ genesis new env dc01 dev
    $ cd dc01/dev
    $ make manifest

Similar to our other deployments, we'll start by using the errors from
``make manifest`` to tell us what values need to be supplied:

::

    $ make manifest
    Found stemcell bosh-openstack-kvm-ubuntu-trusty-go_agent 3312.15 on director
    Found release jumpbox 4.2.3 on director
    Found release toolbelt 3.2.10 on director
    Found release shield 6.3.0 on director
    4 error(s) detected:
     - $.meta.availability_zone: What availability zone should your jumpbox VMs be in?
     - $.networks.jumpbox.subnets: Specify your jumpbox subnet
     - $.properties.jumpbox.users: Set up some users to log into this jumpbox
     - $.properties.shield.agent.autoprovision: What is the URL to this jumpbox's shield installation?


    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

For ``networking.yml``:

::

    ---
    networks:
      - name: jumpbox
        subnets:
          - range: 10.4.16.0/24
            gateway: 10.4.16.1
            dns: [8.8.8.8, 8.8.4.4]
            cloud_properties:
              net_id: 20c35573-3a0c-4725-95a2-b58550407fcf   # <- Dev-Infra-0 Network UUID
            reserved:
              - 10.4.16.2 - 10.4.16.9
              - 10.4.16.20 - 10.4.16.254
            static:
              - 10.4.16.10

      - name: floating
        type: vip
        cloud_properties:
          net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5      # <- Public Network UUID
          security_groups: [wide-open]

    jobs:
      - name: jumpbox
        networks:
        - name: jumpbox
          default: [dns, gateway]
        - name: floating
          static_ips:
          - 172.26.75.116              # <- Floating IP

    cloud_provider:
      properties:
        openstack:
          default_security_groups: [default]

The Floating IP that you generated in Openstack and assign here is what
you will later use to SSH into the jumpbox.

``properties.yml``:

::

    ---
    meta:
      availability_zone: dc01

    properties:
      shield:
        agent:
          autoprovision: https://10.4.1.32    # IP Address for SHIELD VM

The ``credentials.yml`` file is where you'll add your users:

::

    ---
    meta:
      default_ssh_key: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDjqmzJtynAdxmcODCv0dtYmQrYk3lDeb03fjaQTRMtdEYbKnXk9ji8n3CHZj68JxUwxJRtQU70goP7a9X1PVQHJPIYmSWIdWbpeQdTjxymUh+kFjsu6ydlerLpnY3BjR5uWAxoQb+FRKuQDf9+gmkq95M65Lkef0scAXuVJPjLWhYnbZk6hJ4VlZIec6YKfw2x3+8fRM1sDDSWnazIfDQLudAmGnyeaVQsM5Qp7720imRQYPEl6QhwCgNFWhe42BwV5uXqQbBNVlRsoiu9hmGxjZIKB4f4E6uXfw1CPe0ZZh/34/W6CzN+kUwkWSgWet9+kS2Tf9vg0iqQDj/iFz3Cb/kKet0m7EcbYE51Y3fIC2EZAdlp5rQwDgyDoyz+x0IAPRgfMd9DXXjft/7phFdZp1SM4aBQ/bd5oYDpOTxhFZfHSGe4ZCh6tKX2ASzuP7Z4bhGlwZ50RQZqk5iYLsl+4g3Lt4XnjCz2oHgUHM5XVFiGMr7+PBqnQuWrDYJRcRAXwFZNh1dLRcj2ibYcemLWR31RfkYsEDTm6GbdjmV+XHkuvcqnkv7ZHx1MC2FhEELKLY2/LoU+8At8Fk2YU8JAfk9PROnCsQ8GjABZtEGBywHJxUXIMOFmj+9gJeHkbZsDQe6aas1z90HUKfK3u5AU5kC0e62RMjrtwb99eRK2+Q==

    properties:
      jumpbox:
        users:
        - name: juser
          shell: /bin/bash
          setup_script: /var/vcap/packages/ruby-gems/bin/installer
          ssh_keys:
          - (( grab meta.default_ssh_key ))

Here we have the public key for ``bosh`` as the default SSH key. You
could optionally add it to Vault like so:

::

    $ safe set secret/dc01/dev/jumpbox/ssh/default pubkey@/full/path/to/bosh.pub

This creates the entry ``secret/dc01/dev/jumpbox/ssh/default`` with
``pubkey`` set to the contents of the file. You can view the secret with
``safe get secret/dc01/dev/jumpbox/ssh/default:pubkey``.

You can then modify your ``credentials.yml`` as follows:

::

    ---
    properties:
      jumpbox:
        users:
        - name: juser
          shell: /bin/bash
          setup_script: /var/vcap/packages/ruby-gems/bin/installer
          ssh_keys:
          - (( vault meta.vault_prefix "/ssh/default:pubkey" ))

Now that all of the errors are resolved, you can deploy with
``make manifest deploy``. When you need to add additional users, simply
update the ``credentials.yml`` file to mimic the above, whichever route
you have chosen for storing / not storing the public key in Vault.

Note: although you need to also create an alpha (BOSH Lite) jumpbox for
``genesis ci`` / Concourse for the purposes of updating stemcells and
releases, you only need the proto jumpbox and dev jumpboxes to access
all of your environments. The proto jumpbox is intended to access the
proto BOSH and BOSH Lite directors and the beta (dev) jumpbox is
intended to access the dev BOSH director.

Now it's time to move on to deploying our ``beta`` (staging) Cloud
Foundry!

Beta Cloud Foundry
^^^^^^^^^^^^^^^^^^

To deploy Cloud Foundry, we will go back into our ``ops`` directory,
making use of the ``cf-deployments`` repo created when we built our
alpha site:

::

    $ cd ~/ops/cf-deployments

Also, make sure that you're targeting the right Vault, for good measure:

::

    $ safe target proto

We will now create an ``dc01`` site for CF:

::

    $ genesis new site --template openstack dc01
    Created site dc01 (from template openstack):
    ~/ops/cf-deployments/dc01
    ├── README
    └── site
        ├── disk-pools.yml
        ├── jobs.yml
        ├── networks.yml
        ├── properties.yml
        ├── releases
        ├── resource-pools.yml
        ├── stemcell
        │   ├── name
        │   └── version
        └── update.yml

    2 directories, 10 files

And the ``staging`` environment inside it:

::

    $ genesis new env dc01 staging

        proto       https://10.4.1.16:8200

        Use this Vault for storing deployment credentials?  [yes or no] yes
        Generating Cloud Foundry internal certs
        Uploading Cloud Foundry internal certs to Vault
        wrote secret/dc01/staging/cf-deployments/certs/internal_ca
        wrote secret/dc01/staging/cf-deployments/certs/consul_client
        wrote secret/dc01/staging/cf-deployments/certs/consul_server
        wrote secret/dc01/staging/cf-deployments/certs/etcd_client
        wrote secret/dc01/staging/cf-deployments/certs/etcd_server
        wrote secret/dc01/staging/cf-deployments/certs/etcd_peer
        wrote secret/dc01/staging/cf-deployments/certs/blobstore
        wrote secret/dc01/staging/cf-deployments/certs/uaa
        wrote secret/dc01/staging/cf-deployments/certs/bbs_client
        wrote secret/dc01/staging/cf-deployments/certs/bbs
        wrote secret/dc01/staging/cf-deployments/certs/rep_client
        wrote secret/dc01/staging/cf-deployments/certs/rep
        wrote secret/dc01/staging/cf-deployments/certs/doppler
        wrote secret/dc01/staging/cf-deployments/certs/metron
        wrote secret/dc01/staging/cf-deployments/certs/trafficcontroller
        Creating JWT Signing Key
        Creating app_ssh host key fingerprint
        Generating secrets
        Created environment dc01/staging:
        ~/ops/cf-deployments-deployments/dc01/staging
        ├── cloudfoundry.yml
        ├── credentials.yml
        ├── director.yml
        ├── Makefile
        ├── monitoring.yml
        ├── name.yml
        ├── networking.yml
        ├── properties.yml
        ├── README
        └── scaling.yml

        0 directories, 10 files

As you might have guessed, the next step will be to see what parameters
we need to fill in:

::

    $ cd dc01/staging
    $ make manifest
    71 error(s) detected:
     - $.meta.azs.z1: What availability zone should the *_z1 vms be placed in?
     - $.meta.azs.z2: What availability zone should the *_z2 vms be placed in?
     - $.meta.azs.z3: What availability zone should the *_z3 vms be placed in?
     - $.meta.cf.base_domain: Enter the Cloud Foundry base domain
     - $.meta.cf.blobstore_config.fog_connection.aws_access_key_id: What is the access key id for the blobstore S3 buckets?
     - $.meta.cf.blobstore_config.fog_connection.aws_secret_access_key: What is the secret key for the blobstore S3 buckets?
     - $.meta.cf.blobstore_config.fog_connection.host: What is the host name for the blobstore S3 buckets?
     - $.meta.cf.blobstore_config.fog_connection.port: What is the port for the blobstore S3 buckets?
     - $.meta.cf.directory_key_prefix: Replace the period in CF base domain with dash
     - $.meta.dns: Enter the DNS server for your VPC
     - $.meta.router_security_groups: Enter the security groups which should be applied to the gorouter VMs
     - $.meta.security_groups: Enter the security groups which should be applied to CF VMs
     - $.networks.cf1.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
     - $.networks.cf1.subnets.0.gateway: Enter the Gateway for this subnet
     - $.networks.cf1.subnets.0.range: Enter the CIDR address for this subnet
     - $.networks.cf1.subnets.0.reserved: Enter the reserved IP ranges for this subnet
     - $.networks.cf1.subnets.0.static: Enter the static IP ranges for this subnet
     - $.networks.cf2.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
     - $.networks.cf2.subnets.0.gateway: Enter the Gateway for this subnet
     - $.networks.cf2.subnets.0.range: Enter the CIDR address for this subnet
     - $.networks.cf2.subnets.0.reserved: Enter the reserved IP ranges for this subnet
     - $.networks.cf2.subnets.0.static: Enter the static IP ranges for this subnet
     - $.networks.cf3.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
     - $.networks.cf3.subnets.0.gateway: Enter the Gateway for this subnet
     - $.networks.cf3.subnets.0.range: Enter the CIDR address for this subnet
     - $.networks.cf3.subnets.0.reserved: Enter the reserved IP ranges for this subnet
     - $.networks.cf3.subnets.0.static: Enter the static IP ranges for this subnet
     - $.networks.router1.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
     - $.networks.router1.subnets.0.gateway: Enter the Gateway for this subnet
     - $.networks.router1.subnets.0.range: Enter the CIDR address for this subnet
     - $.networks.router1.subnets.0.reserved: Enter the reserved IP ranges for this subnet
     - $.networks.router1.subnets.0.static: Enter the static IP ranges for this subnet
     - $.networks.router2.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
     - $.networks.router2.subnets.0.gateway: Enter the Gateway for this subnet
     - $.networks.router2.subnets.0.range: Enter the CIDR address for this subnet
     - $.networks.router2.subnets.0.reserved: Enter the reserved IP ranges for this subnet
     - $.networks.router2.subnets.0.static: Enter the static IP ranges for this subnet
     - $.networks.runner1.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
     - $.networks.runner1.subnets.0.gateway: Enter the Gateway for this subnet
     - $.networks.runner1.subnets.0.range: Enter the CIDR address for this subnet
     - $.networks.runner1.subnets.0.reserved: Enter the reserved IP ranges for this subnet
     - $.networks.runner1.subnets.0.static: Enter the static IP ranges for this subnet
     - $.networks.runner2.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
     - $.networks.runner2.subnets.0.gateway: Enter the Gateway for this subnet
     - $.networks.runner2.subnets.0.range: Enter the CIDR address for this subnet
     - $.networks.runner2.subnets.0.reserved: Enter the reserved IP ranges for this subnet
     - $.networks.runner2.subnets.0.static: Enter the static IP ranges for this subnet
     - $.networks.runner3.subnets.0.cloud_properties.net_id: Enter the OpenStack Network ID for this subnet
     - $.networks.runner3.subnets.0.gateway: Enter the Gateway for this subnet
     - $.networks.runner3.subnets.0.range: Enter the CIDR address for this subnet
     - $.networks.runner3.subnets.0.reserved: Enter the reserved IP ranges for this subnet
     - $.networks.runner3.subnets.0.static: Enter the static IP ranges for this subnet
     - $.properties.cc.buildpacks.fog_connection.aws_access_key_id: What is the access key id for the blobstore S3 buckets?
     - $.properties.cc.buildpacks.fog_connection.aws_secret_access_key: What is the secret key for the blobstore S3 buckets?
     - $.properties.cc.buildpacks.fog_connection.host: What is the host name for the blobstore S3 buckets?
     - $.properties.cc.buildpacks.fog_connection.port: What is the port for the blobstore S3 buckets?
     - $.properties.cc.droplets.fog_connection.aws_access_key_id: What is the access key id for the blobstore S3 buckets?
     - $.properties.cc.droplets.fog_connection.aws_secret_access_key: What is the secret key for the blobstore S3 buckets?
     - $.properties.cc.droplets.fog_connection.host: What is the host name for the blobstore S3 buckets?
     - $.properties.cc.droplets.fog_connection.port: What is the port for the blobstore S3 buckets?
     - $.properties.cc.packages.fog_connection.aws_access_key_id: What is the access key id for the blobstore S3 buckets?
     - $.properties.cc.packages.fog_connection.aws_secret_access_key: What is the secret key for the blobstore S3 buckets?
     - $.properties.cc.packages.fog_connection.host: What is the host name for the blobstore S3 buckets?
     - $.properties.cc.packages.fog_connection.port: What is the port for the blobstore S3 buckets?
     - $.properties.cc.resource_pool.fog_connection.aws_access_key_id: What is the access key id for the blobstore S3 buckets?
     - $.properties.cc.resource_pool.fog_connection.aws_secret_access_key: What is the secret key for the blobstore S3 buckets?
     - $.properties.cc.resource_pool.fog_connection.host: What is the host name for the blobstore S3 buckets?
     - $.properties.cc.resource_pool.fog_connection.port: What is the port for the blobstore S3 buckets?
     - $.properties.cc.security_group_definitions.load_balancer.rules: Specify the rules for allowing access for CF apps to talk to the CF Load Balancer External IPs
     - $.properties.cc.security_group_definitions.services.rules: Specify the rules for allowing access to CF services subnets
     - $.properties.cc.security_group_definitions.user_bosh_deployments.rules: Specify the rules for additional BOSH user services that apps will need to talk to


    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

Oh boy. That's a lot. Cloud Foundry must be complicated. Looks like a
lot of the fog\_connection properties are all duplicates though, so lets
fill out ``properties.yml`` with those (no need to create the blobstore
S3 buckets yourself):

``properties.yml``

::

    ---
    meta:
      type: cf
      site: dc01
      env: dev
      skip_ssl_validation: true

      cf:
        base_domain: 192.168.10.154.sslip.io
        directory_key_prefix: 192-168-10-154-sslip-io
        blobstore_config:
          fog_connection:
            aws_access_key_id: (( vault "secret/s3:access_key" ))
            aws_secret_access_key: (( vault "secret/s3:secret_key" ))
            scheme: http
            # host: (( get "object." + meta.openstack.domain ))
            host: 192.168.8.168
            port: 8080
            # Required
            path_style: true
            # v4 is buggy at the moment, stick with v2 for now
            aws_signature_version: 2
            provider: "AWS"
            #region: dc01
    properties:
      bolo:
        submission:
          address: 10.4.1.65
        collectors:
          - { every: 20s, run: 'linux' }
      loggregator:
        servers:
        - 10.4.20.105
      loggregator_endpoint:
        host: 10.4.20.105
        port: 3456

Also, let's fill out ``scaling.yml`` so we can more easily scale out our
Availability Zones and jobs:

::

    meta:
      azs:
        z1: dc01
        z2: dc01
        z3: dc01
    jobs:
     - name: access_z1
       instances: 1
     - name: access_z2
       instances: 0
     - name: api_z1
       instances: 1
     - name: api_z2
       instances: 0
     - name: brain_z1
       instances: 1
     - name: brain_z2
       instances: 0
     - name: cc_bridge_z1
       instances: 1
     - name: cc_bridge_z2
       instances: 0
     - name: cell_z1
       instances: 1
     - name: cell_z2
       instances: 0
     - name: doppler_z1
       instances: 1
     - name: doppler_z2
       instances: 0
     - name: loggregator_trafficcontroller_z1
       instances: 1
     - name: loggregator_trafficcontroller_z2
       instances: 0
     - name: route_emitter_z1
       instances: 1
     - name: route_emitter_z2
       instances: 0
     - name: router_z1
       instances: 1
     - name: router_z2
       instances: 0
     - name: stats
       instances: 1
     - name: uaa_z1
       instances: 1
     - name: uaa_z2
       instances: 0

Time to start building out the ``networking.yml`` file. Let's consult
our `Network
Plan <https://github.com/starkandwayne/codex/blob/master/network.md>`__
for the subnet information, cross referencing with Terraform output to
get the subnet IDs:

::

    ---
    meta:
      azs:
        z1: dc01
        z2: dc01
        z3: dc01
      dns: [8.8.8.8, 8.8.4.4]
      router_security_groups: [wide-open]
      security_groups: [wide-open]

    networks:
    - name: router1
      subnets:
      - range: 10.4.19.0/25
        static: [10.4.19.4 - 10.4.19.10]
        reserved:
          - 10.4.19.2 - 10.4.19.3
          - 10.4.19.120 - 10.4.19.126
        gateway: 10.4.19.1
        cloud_properties:
          net_id: 262fb235-de6c-4979-832a-225c66859d26
    - name: router2
      subnets:
      - range: 10.4.19.128/25
        static: [10.4.19.132 - 10.4.19.138]
        reserved:
          - 10.4.19.130 - 10.4.19.131
          - 10.4.19.248 - 10.4.19.254
        gateway: 10.4.19.129
        cloud_properties:
          net_id: 41fb3f7d-9198-49e2-84b9-d142628a666a
    - name: cf1
      subnets:
      - range: 10.4.20.0/24
        static: [10.4.20.4 - 10.4.20.100]
        reserved: [10.4.20.2 - 10.4.20.3]
        gateway: 10.4.20.1
        cloud_properties:
          net_id: bbea24c9-58dc-4df5-899d-fd46e3dfbe5e
    - name: cf2
      subnets:
      - range: 10.4.21.0/24
        static: [10.4.21.4 - 10.4.21.100]
        reserved: [10.4.21.2 - 10.4.21.3]
        gateway: 10.4.21.1
        cloud_properties:
          net_id: a2f1e561-2c02-4f7b-9dd9-4c5fd44c9783
    - name: cf3
      subnets:
      - range: 10.4.22.0/24
        static: [10.4.22.4 - 10.4.22.100]
        reserved: [10.4.22.2 - 10.4.22.3]
        gateway: 10.4.22.1
        cloud_properties:
          net_id: 4e1df0ff-f7f9-4cf8-9c19-77afd48e7e9f
    - name: runner1
      subnets:
      - range: 10.4.23.0/24
        static: [10.4.23.4 - 10.4.23.100]
        reserved: [10.4.23.2 - 10.4.23.3]
        gateway: 10.4.23.1
        cloud_properties:
          net_id: 7f4b6f05-685f-48c7-bc6d-edc8bd00b145
    - name: runner2
      subnets:
      - range: 10.4.24.0/24
        static: [10.4.24.4 - 10.4.24.100]
        reserved: [10.4.24.2 - 10.4.24.3]
        gateway: 10.4.24.1
        cloud_properties:
          net_id: 631b5c2d-7948-4e77-8ca7-36417514e835
    - name: runner3
      subnets:
      - range: 10.4.25.0/24
        static: [10.4.25.4 - 10.4.25.100]
        reserved: [10.4.25.2 - 10.4.25.3]
        gateway: 10.4.25.1
        cloud_properties:
          net_id: ecbf813d-77af-4919-9c80-279d9eaf10c6

    - name: floating
      type: vip
      cloud_properties:
        net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5
        security_groups: [wide-open]
    jobs:
    - name: api_z1
      networks:
      - name: cf1
        default: [dns, gateway]
      - name: floating
        static_ips:
        - 192.168.10.125
    - name: api_z2
      networks:
      - name: cf2
        default: [dns, gateway]

    properties:
      cc:
        security_group_definitions:
        - name: load_balancer
          rules: []
        - name: services
          rules:
          - destination: 10.4.26.0-10.4.28.255
            protocol: all
        - name: user_bosh_deployments
          rules: []

That should be it, finally. Let's deploy!

::

    $ make deploy
    RSA 1024 bit CA certificates are loaded due to old openssl compatibility
    Acting as user 'admin' on 'dc01-staging-bosh'
    Checking whether release cf/250 already exists...NO
    Using remote release 'https://bosh.io/d/github.com/cloudfoundry/cf-release?v=250'

    Director task 6
      Started downloading remote release > Downloading remote release
    ...
    Deploying
    ---------
    Are you sure you want to deploy? (type 'yes' to continue): yes
    ...

    Started     2017-01-02 17:23:47 UTC
    Finished    2017-01-02 17:34:46 UTC
    Duration    00:10:59

    Deployed 'dc01-staging-cf' to 'dc01-staging-bosh'

If you want to scale your deployment in the current environment (here it
is staging), you can modify ``scaling.yml`` in your
``cf-deployments/dc01/staging`` directory. In the following example, you
scale runners in both AZ to 2. Afterwards you can run ``make manifest``
and ``make deploy``, but always remember to verify your changes in the
manifest before you type ``yes``.

::

    jobs:

    - name: runner_z1
      instances: 2

    - name: runner_z2
      instances: 2

To make the manifest and deploy the changes run
``make manifest deploy``. Always make sure to verify the detected
changes match what you intended in the manifest before entering ``yes``
to kickoff the deploy.

After a long while of compiling and deploying VMs, your Cloud Foundry
should now be up, and accessible! You can check the sanity by running
the smoke tests with ``genesis bosh run errand smoke_tests``.

To target your Cloud Foundry to start making orgs, spaces, and pushing
apps use:

::

    cf login -a https://api.system.192.168.10.154.sslip.io

The admin user's password can be retrieved from Vault with
``safe get secret/dc01/dev/cf-cloudfoundry/creds/users/admin:password``.
If you run into any trouble, make sure that your DNS is pointing
properly to the correct Load Balancer for this environment and that the
Load Balancer has the correct SSL certificate for your site.

Push An App to Beta Cloud Foundry
'''''''''''''''''''''''''''''''''

After you successfully deploy the Beta CF, you can push an simple app to
learn more about CF. In the CF world, every application and service is
scoped to a space. A space is inside an org and provides users with
access to a shared location for application development, deployment, and
maintenance. An org is a development account that an individual or
multiple collaborators can own and use. You can click `orgs, spaces,
roles and
permissions <https://docs.cloudfoundry.org/concepts/roles.html>`__ to
learn more details.

The first step is creating and org and an space and targeting the org
and space you created by running the following commands.

::

    cf create-org sw-codex
    cf target -o sw-codex
    cf create-space test
    cf target -s test

Once you are in the space, you can push an very simple app
`cf-env <https://github.com/cloudfoundry-community/cf-env>`__ to the CF.
Clone the `cf-env <https://github.com/cloudfoundry-community/cf-env>`__
repo on your bastion server, then go inside the ``cf-env`` directory,
simply run ``cf push`` and it will start to upload, stage and run your
app.

Your ``cf push`` command may fail like this:

::

    Using manifest file /home/user/cf-env/manifest.yml

    Updating app cf-env in org sw-codex / space test as admin...
    OK

    Uploading cf-env...
    FAILED
    Error processing app files: Error uploading application.
    Server error, status code: 500, error code: 10001, message: An unknown error occurred.

You can try to debug this yourself for a while or find the possible
solution in `Debug Unknown Error When You Push Your APP to
CF <http://www.starkandwayne.com/blog/debug-unknown-error-when-you-push-your-app-to-cf/>`__.

Beta haproxy
^^^^^^^^^^^^

For the haproxy/(-ies) in beta environments you'll need to create a
Floating IP for each environment. For this deployment, we'll be using
``192.168.10.154`` as the Floating IP.

Once you have the Floating IP, create the new environment:

::

    $ cd ~/ops/haproxy-deployments
    $ genesis new site --template openstack dc01
    $ genesis new dc01 dev
    $ cd dc01/dev
    $ make manifest

::

    ...
    8 error(s) detected:
     - $.meta.azs.z1: What availability zone should the *_z1 vms be placed in?
     - $.meta.azs.z2: What availability zone should the *_z2 vms be placed in?
     - $.meta.azs.z3: What availability zone should the *_z3 vms be placed in?
     - $.networks.haproxy1.subnets: Please specify haproxy1 subnets
     - $.networks.haproxy2.subnets: Please specify haproxy1 subnets
     - $.properties.ha_proxy.backend_servers: Specify your go routers IPs as backend servers
     - $.properties.ha_proxy.ssl_pem: Configure the ssl pem you use for your haproxy
     - $.properties.ha_proxy.tcp.cf_app_ssh.backend_servers: Specify you  CF Access VMs IPs as your backend servers

Now add the network information to ``networking.yml``:

::

    --
    meta:
      azs:
        z1: dc01
        z2: (( grab meta.azs.z1 ))
        z3: (( grab meta.azs.z1 ))
      dns: [8.8.8.8, 8.8.4.4]
      router_security_groups: [wide-open]
      security_groups: [wide-open]

    networks:
    - name: haproxy1
      type: manual
      subnets:
      - range: 10.4.19.0/25
        reserved:
        - 10.4.19.2 - 10.4.19.8
        static: [ 10.4.19.9 - 10.4.19.12 ]
        gateway: 10.4.19.1
        cloud_properties:
          net_id: 262fb235-de6c-4979-832a-225c66859d26    # Network UUID for dev-cf-edge-0
          security_groups: [ wide-open ]

    - name: haproxy2
      type: manual
      subnets:
      - range: 10.4.19.128/25
        reserved:
        - 10.4.19.129 - 10.4.19.135
        static: [ 10.4.19.136 - 10.4.19.140 ]
        gateway: 10.4.19.129
        cloud_properties:
          net_id: 41fb3f7d-9198-49e2-84b9-d142628a666a    # Network UUID for dev-cf-edge-1
          security_groups: [ wide-open ]

    - name: floating
      type: vip
      cloud_properties:
        net_id: 09b03d93-45f8-4bea-b3b8-7ad9169f23d5      # Network UUID for public
        security_groups: [wide-open]

    jobs:
      - name: haproxy_z1
        networks:
        - name: haproxy1
          default: [dns, gateway]
        - name: floating
          static_ips:
          - 192.168.10.154                                 # Floating IP for haproxy

And also the ``properties.yml``:

::

    ---
    properties:
      ha_proxy:
        ssl_pem:
          - (( vault meta.vault_prefix "/certs/pems:192.168.10.154.sslip.io" ))
        backend_servers: [ 10.4.19.4, 10.4.19.132 ]  # Dev Cloud Foundry router_z* IPs

        tcp:
          - name: cf_app_ssh
            backend_servers: [ 10.4.20.108 ]         # Dev Cloud Foundry access_z1 IP

As before, we'll need to generate the haproxy cert. In this case, we
have a true Floating IP so we will be using that IP to generate the
cert:

::

    $ ENV_PATH=secret/dc01/dev FIP=192.168.10.154 ./bin/haproxy_cert_gen

As a reminder if you are using a domain with its own CA, you will
generate the cert with your own internal process and upload it to Vault
using ``safe write``. For example, if you had a cert for ``example.com``
for this environment and saved it as ``haproxy.pem``, you would use:

::

    $ safe write secret/dc01/dev/haproxy/certs/pems example.com@haproxy.pem

You would then reference the secret in the manifest with
``(( vault meta.vault_prefix "/certs/pems:example.com" ))``.

After configuring all the templates, and generating the certificate,
make the manifest and deploy with ``make manifest deploy``.

There is one final step: since Cloud Foundry was already deployed so we
could use it's CA, you'll need to edit ``properties.yml`` haproxy
Floating IP for the base domain and directory key prefix:

::

      cf:
        base_domain: 192.168.10.154.sslip.io
        directory_key_prefix: 192-168-10-154-sslip-io

Once this is done remake the manifest and deploy the Beta (dev) Cloud
Foundry with ``make manifest deploy``. You will also need to delete the
original ``run`` and ``system`` domains by listing the domains with
``cf domains`` and then using ``cf delete-shared-domain`` and
``cf delete-domain``. Alternatively, if the Cloud Foundry has yet to
have any apps/etc. added to it you may want to delete the deployment
with ``bosh delete deployment dc01-dev-cf-cloudfoundry`` and then deploy
normally.

Production Environment
~~~~~~~~~~~~~~~~~~~~~~

Deploying the production environment will be much like deploying the
``beta`` environment above. You will need to deploy a BOSH Director,
Cloud Foundry, and any services also deployed in the ``beta`` site.
Hostnames, credentials, network information, and possibly scaling
parameters will all be different, but the procedure for deploying them
is the same.

Logging with Sawmill
--------------------

Sawmill is a BOSH release that aggregates logs to assist with
troubleshooting Cloud Foundry and its services that support
`nxlog <https://github.com/hybris/nxlog-boshrelease>`__. Although
Sawmill logs can be viewed using ``curl``, we recommend installing
``logemongo`` - a CLI tool for Sawmill. If you need to store your logs
for later access, we recommend using a storage bucket (e.g. S3).

Since we only have Cloud Foundries in Alpha (BOSH Lite) and Dev, we'll
only be deploying Sawmill to those two environments here.

How to deploy
~~~~~~~~~~~~~

To create the deployment:

::

    $ genesis new deployment --template sawmill
    $ cd sawmill-deployments

For the Beta (Dev) Environment:

::

    $ genesis new site --template openstack dc01
    $ genesis new env dc01 dev
    $ cd dc01/dev
    $ make manifest

As before, we'll need to supply the information requested in the errors:

::

    $ make manifest
    Found stemcell bosh-openstack-kvm-ubuntu-trusty-go_agent latest on director
    Found release sawmill 2.1.0 on director
    Found release toolbelt latest on director
    8 error(s) detected:
     - $.meta.openstack.azs.z1: Define the z1 Openstack availability zone
     - $.networks.sawmill_z1.subnets.0.cloud_properties.net_id: Enter the network ID Openstack for the Dev-Infra-0 network (networks.0<sawmill_z1>.subnets.cloud_properties.net_id))
     - $.networks.sawmill_z1.subnets.0.gateway: Enter the Gateway for this subnet (networks.0<sawmill_z1>.subnets.gateway)
     - $.networks.sawmill_z1.subnets.0.range: Enter the CIDR address for this subnet (networks.0<sawmill_z1>.subnets.range)
     - $.networks.sawmill_z1.subnets.0.reserved: Enter the reserved IP ranges for this subnet (networks.0<sawmill_z1>.subnets.reserved)
     - $.networks.sawmill_z1.subnets.0.static: Enter the static IP ranges for this subnet (networks.0<sawmill_z1>.subnets.static)
     - $.properties.sawmill.skip_ssl_verify: Specify whether or not to skip SSL verification (properties.sawmill.skip_ssl_verify)
     - $.properties.sawmill.users: Specify admin user name & password (properties.sawmill.users.0.{name,pass})


    Failed to merge templates; bailing...
    Makefile:22: recipe for target 'manifest' failed
    make: *** [manifest] Error 5

``networking.yml``:

::

    ---
    meta:
      openstack:
        azs:
          z1: dc01

    networks:
      - name: sawmill_z1
        type: manual
        subnets:
        - range: 10.4.16.0/24
          gateway: 10.4.16.1
          dns: [8.8.8.8, 8.8.4.4]
          reserved:
            - 10.4.16.2 - 10.4.16.19
            - 10.4.16.30 - 10.4.16.254
          static:
            - 10.4.16.20
          cloud_properties:
            net_id: 20c35573-3a0c-4725-95a2-b58550407fcf   # <- Dev-Infra-0 Network UUID
            security_groups: [wide-open]

``credentials.yml``:

::

    ---
    properties:
      sawmill:
        users:
          - name: admin
            pass: (( vault meta.vault_prefix "/users/admin:password" ))

``properties.yml``:

::

    ---
    properties:
      sawmill:
        skip_ssl_verify: true

You will need to add the user to Vault and assign a password. The
easiest way to do this is:

::

    safe gen secret/dc01/dev/sawmill/users/admin password

You can view the sawmill credentials tree in Vault with ``safe``:

::

    safe tree secret/dc01/dev/sawmill

You can view the password with
``safe get secret/dc01/dev/sawmill/users/admin:password``. Now you can
make the manifest and deploy:

::

    $ make manifest deploy

For the **Alpha** (BOSH Lite) Environment:

Repeat the above process for both alpha (BOSH Lite) and dev. Since BOSH
Lite is it's own site, and not in ``dc01``, make sure to use the
``bosh-lite`` template for that deployment e.g. assuming you're in the
``sawmill-deployments`` directory:

::

    $ genesis new site --template bosh-lite bosh-lite
    $ genesis new env bosh-lite alpha
    $ cd bosh-lite/alpha
    $ make manifest

In this case both ``credentials.yml`` and ``properties.yml`` will match
the Dev deployment, so we only need to add ``networking.yml``:

::

    ---
    networks:
      - name: sawmill_z1
        type: manual
        subnets:
        - range: 10.244.8.0/24
          gateway: 10.244.8.1
          dns: [8.8.8.8, 8.8.4.4]
          reserved:
            - 10.244.8.2 - 10.244.8.19
            - 10.244.8.30 - 10.244.8.254
          static:
            - 10.244.8.20

And create the ``user:password`` in Vault:

::

    safe gen secret/bosh-lite/alpha/sawmill/users/admin password

Now you can make the manifest and deploy:

::

    $ make manifest deploy

Viewing Logs with Logemongo
~~~~~~~~~~~~~~~~~~~~~~~~~~~

First, log into a jumpbox that can access the private network of the
environment where you want to view logs (e.g. the dev jumpbox for the
dev environment). Then, grab ``[logemongo](logemongo)`` from Github:

::

    sudo curl -o /usr/local/bin/logemongo \
      https://raw.githubusercontent.com/starkandwayne/logemongo/master/logemongo
    sudo chmod 0755 /usr/local/bin/logemongo

Since our jumpboxes use Ubuntu, we'll need to grab a couple packages for
the utility:

::

    sudo apt-get install libio-socket-ssl-perl libconfig-yaml-perl

You can view the streaming logs from the appropriate Sawmill using the
host flag and passing through the username and password. Since we're
using self-signed certificates, we'll also need to disable SSL
verification. e.g. to stream logs in dev:

::

    $ logemongo -H 10.4.16.20 -u admin -p $(safe get secret/dc01/dev/sawmill/users/admin:password) --no-ssl

Depending on log volume, it may take a few moments for logs to begin to
appear. If you wish to stream logs only from a certain source, or
exclude logs only from a certain source, you can use ``-i`` and ``-x``
respectively with a regex pattern. You can also limit the number of
lines of output with ``-c``.

Next Steps
~~~~~~~~~~

Lather, rinse, repeat for all additional environments (e.g. loadtest,
production).
